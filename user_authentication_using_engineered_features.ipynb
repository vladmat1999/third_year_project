{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One class SVM model for intruder detection\n",
    "This notebook contains code to run experiments with a 1-class SVM model for user authentiation. \n",
    "\n",
    "The experiments run multithreaded and output the result to a specified directory. You can change the output directory by changing the variable `OUTPUT_DIR` variable, and set the number of threads using the `N_THREADS` variable. Using more threads will increase memory demands, so limit to 1 thread/GB of avilable memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for the experiments\n",
    "Set the parameters with which the experiment will be run. Below is a description for each of them.\n",
    "* `DATASET`: Specify which dataset to be used. Choose between BRAINRUN and TOUCHALYTICS. The paths for the dataset are assumed to be at `./datasets`, and can be changed in the `config.py` file.\n",
    "* `N_THREADS`: Number of threads to use for the experiment.\n",
    "* `OUTPUT_DIR`: Directory to which the results will be written.\n",
    "* `USE_PRESSURE_AND_AREA_FEATURES`: Whether to use pressure and area features - only for the *Touchalytics* dataset, is ignored when using the *BrainRun* dataset.\n",
    "\n",
    "Here we also set the parameters for data cleanup and parsing, although these are best left constant.\n",
    "* `MIN_SESSION_GESTURES`: Minimum number of gestures in a session to be considered. Left at 140 as it has shown to produce good results, and include a large number of users\n",
    "* `SCREENS`: The screens to use from the BrainRun dataset when performing the experiment (ignored for the Touchalytics dataset). The experiments were originally performed using either one or both of the screens *MathisisGame* or *FocusGame*, as they contain predominantly swipe data.\n",
    "* `WINDOW_SIZE`: Left at 1 constant throught these experiments, as majority voting over sliding windows is employed later to verify the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = TOUCHALYTICS # Choose between BRAINRUN and TOUCHALYTICS\n",
    "USE_PRESSURE_AND_AREA_FEATURES = True\n",
    "N_THREADS = 32\n",
    "OUTPUT_DIR = 'test_results/'\n",
    "\n",
    "MIN_SESSION_GESTURES = 140\n",
    "SCREENS = ['MathisisGame', 'FocusGame']\n",
    "WINDOW_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods for extracting features and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, delta_time):\n",
    "    '''\n",
    "    Extracts manually  engineered features from a gesture (swipe). \n",
    "    \n",
    "    Returns an array of length 23 (25 if area and pressure are used) with\n",
    "    the extracted features\n",
    "    '''\n",
    "    result = []\n",
    "    points = [(data[0]['x0'], data[0]['y0'])] + [(x['moveX'], x['moveY']) for x in data]\n",
    "    points = np.array(points)\n",
    "\n",
    "    # End to end distance - from paper (Touchalytics) and https://par.nsf.gov/servlets/purl/10167262\n",
    "    result.append(np.linalg.norm(points[-1] - points[0]))\n",
    "    # Path length - from Touchalytics adn https://par.nsf.gov/servlets/purl/10167262\n",
    "    result.append(sum(np.linalg.norm(points[i] - points[i - 1]) for i in range(1, len(points))))\n",
    "    # Path length / delta time - from https://par.nsf.gov/servlets/purl/10167262\n",
    "    result.append(result[-1] / delta_time)\n",
    "    # Average Acceleration\n",
    "    # result.append(result[-1] / delta_time)\n",
    "\n",
    "    # 20th percentile velocity - Touchalytics\n",
    "    result.append(\n",
    "        np.percentile(\n",
    "            [np.linalg.norm(np.array([current_point['vx'], current_point['vy']])) for current_point in data],\n",
    "            20\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 80th percentile velocity - Touchalytics\n",
    "    result.append(\n",
    "        np.percentile(\n",
    "            [np.linalg.norm(np.array([current_point['vx'], current_point['vy']])) for current_point in data],\n",
    "            80\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Angle (direction of end-to-end line) - Touchalytics\n",
    "    result.append(np.angle(\n",
    "        complex(points[-1][0] - points[0][0], points[-1][1] - points[0][1])\n",
    "    ))\n",
    "\n",
    "    # Largest distance from end-to-end line (signed) - Touchalytics\n",
    "    result.append(\n",
    "        np.max(\n",
    "            [np.cross(points[-1] - points[0], points[0] - p) / (np.linalg.norm(points[-1] - points[0]) or 1) for p in points]\n",
    "        ))\n",
    "\n",
    "    # Largest deviation from end to end line\n",
    "    result.append(max(abs(np.arctan2(*(points[i] - points[i - 1]))) for i in range(1, len(points))))\n",
    "\n",
    "    # First 3 point acceleration\n",
    "    result.append(np.linalg.norm(2 * points[1] - points[0] - points[2]) / 2 if len(points) > 3 else 0)\n",
    "    # Last 3 point acceleration\n",
    "    result.append(np.linalg.norm(2 * points[-2] - points[-3] - points[-1]) / 2 if len(points) > 3 else 0)\n",
    "\n",
    "    # Median velocity at last three points - Touchalytics (11 features + duration = 12 - without the first and last 3 points)\n",
    "    result.append(\n",
    "        np.median(\n",
    "            np.array(\n",
    "                [np.linalg.norm(np.array([data[i]['vx'], data[i]['vy']])\n",
    "                ) for i in range(len(data) - 1, max(-1, len(data) - 4), -1)])))\n",
    "\n",
    "    # Only for touchalytics\n",
    "    if USE_PRESSURE_AND_AREA_FEATURES and DATASET == TOUCHALYTICS:\n",
    "        # Midstroke area covered\n",
    "        result.append(data[int(len(data) // 2)]['area'])\n",
    "        # Midstroke pressure\n",
    "        result.append(data[int(len(data) // 2)]['pressure'])\n",
    "\n",
    "    res = np.nan_to_num(result)\n",
    "\n",
    "    first_three_points = points[:3].flatten() \n",
    "    last_three_points = points[-3:].flatten() \n",
    "\n",
    "    first_three_points.resize((6,))\n",
    "    last_three_points.resize((6,))\n",
    "\n",
    "    return np.concatenate([res, first_three_points, last_three_points])\n",
    "    \n",
    "def gesture_to_data(c):\n",
    "    # Stroke duration\n",
    "    delta_time = (c['t_stop'] - c['t_start']) / 1000\n",
    "    extra_features = extract_features(c['data'], delta_time)\n",
    "\n",
    "    # Start and stop time are removed when sliding windows is called, and transformed in relative time\n",
    "    # between the gestures in a window\n",
    "    return np.concatenate([[c['t_start'], c['t_stop'], delta_time], extra_features])\n",
    "\n",
    "def window_to_datapoint(window):\n",
    "    return np.concatenate([\n",
    "        window[:, 2:].flatten(), # Exclude start and stop time\n",
    "        (window[1:, 0] - window[0, 1]).flatten() / 1000, # Window start - initial point stop (time from window start for each gesture)\n",
    "        (window[1:, 0] - window[:-1, 1]).flatten() / 1000]) # Window start - previous window stop (pairwise time between gestures)\n",
    "\n",
    "def session_to_datapoints(s):\n",
    "    '''\n",
    "    Converts a session to a series of datapoints. Also slides windows of length WINDOW_SIZE over the gestures\n",
    "    and considers each window as a datapoint. \n",
    "    '''\n",
    "    featurized_session = np.array([gesture_to_data(x) for x in s['gestures']])\n",
    "    sliding_windows = (\n",
    "        np.expand_dims(np.arange(WINDOW_SIZE), 0) +\n",
    "        np.expand_dims(np.arange(len(featurized_session) - WINDOW_SIZE), 0).T\n",
    "    )\n",
    "\n",
    "    return np.array([window_to_datapoint(window) for window in featurized_session[sliding_windows]])\n",
    "\n",
    "def get_train_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = np.inf):\n",
    "    '''\n",
    "    Returns the train indices for a given session.\n",
    "    Leaves a space of `gap` between the train and test indices\n",
    "    '''\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    middle = min(middle, max_size)\n",
    "    return np.arange(middle - gap)\n",
    "\n",
    "def get_test_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = np.inf):\n",
    "    '''\n",
    "    Returns the test indices for a given session.\n",
    "    '''\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    return np.arange(middle, size)\n",
    "\n",
    "def get_intruder_size(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = np.inf):\n",
    "    '''\n",
    "    Returns validation and test indices for intruders. Also leaves a space of `gap` \n",
    "    between the validation and test indices.\n",
    "    '''\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    validation_middle = int(size * (1 - test_size / 2))\n",
    "    return np.arange(middle, validation_middle - gap), np.arange(validation_middle, size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for filtering and parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefilter_session(s):\n",
    "    '''\n",
    "    Filters the session, orders gestures chronologically and removes gestures that are outliers or from different screens\n",
    "    '''\n",
    "    s['gestures'].sort(key = lambda x: x['t_start'])\n",
    "    s['gestures'] = [x for x in s['gestures'] \n",
    "        if x['t_stop'] - x['t_start'] > 70 and x['t_stop'] - x['t_start'] < (1000 if DATASET == BRAINRUN else 2000) and \n",
    "        ((x['screen'].split(' ')[0] in SCREENS and x['type'] == 'swipe') if DATASET == BRAINRUN else True)]\n",
    "\n",
    "def parse_user(user_id):\n",
    "    '''\n",
    "    Parses all the sessions for a user with the given id. Deletes sessions that are too short after filtering them.\n",
    "    '''\n",
    "    i = 0\n",
    "    should_delete = False\n",
    "    while i < len(users[user_id]['devices'][0]['sessions']):\n",
    "        prefilter_session(users[user_id]['devices'][0]['sessions'][i])\n",
    "\n",
    "        if len(users[user_id]['devices'][0]['sessions'][i]['gestures']) < MIN_SESSION_GESTURES or should_delete:\n",
    "            del users[user_id]['devices'][0]['sessions'][i]\n",
    "        else:\n",
    "            users[user_id]['devices'][0]['sessions'][i] = session_to_datapoints(users[user_id]['devices'][0]['sessions'][i])\n",
    "            # Outlier detection\n",
    "            clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "            users[user_id]['devices'][0]['sessions'][i] = \\\n",
    "                users[user_id]['devices'][0]['sessions'][i][np.where(clf.fit_predict(users[user_id]['devices'][0]['sessions'][i]) == 1)]\n",
    "            i += 1\n",
    "\n",
    "def get_users_over_gestures(number_of_gestures = 140):\n",
    "    '''\n",
    "    Returns an array with the indices of all users with more than number_of_gestures gestures.\n",
    "    '''\n",
    "    uc = np.zeros((len(users), ))\n",
    "    for i in range(len(users)):\n",
    "        uc[i] = 0\n",
    "        for session in users[i]['devices'][0]['sessions']:\n",
    "            uc[i] += session.shape[0]\n",
    "\n",
    "    return np.where(uc > number_of_gestures)[0]\n",
    "\n",
    "def compute_eer(label, pred):\n",
    "    \"\"\"\n",
    "    Computes EER given a list of labels and predictions.\n",
    "\n",
    "    Code inspired by https://github.com/YuanGongND/python-compute-eer\n",
    "    \"\"\"\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for multithreaded experiments\n",
    "\n",
    "Since the datasets are large and a iteration is done for each users, the experiments take a significant amount of time to run on a normal machine. Multiprocessing was used to run the experiments in parallel on a powerful machine, using 32 cores (this reduces the time to about an hour). Below is the code used to run the experiments using different processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter space (for hyperparameter tuning)\n",
    "parameters = [{\n",
    "    'kernel': ['rbf'], 'gamma': [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001], 'nu': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],},\n",
    "    ]\n",
    "\n",
    "def run_experiment_multithreaded(output_path, dataset, valid_users):\n",
    "    FILE_PATH = f'{output_path}/{dataset}'\n",
    "\n",
    "    def run_experiment_for_user(user_id, users, valid_users, parameters):\n",
    "        max_train_per_session = np.inf\n",
    "\n",
    "        # Uncomment this line to test the effect of different number of gestures\n",
    "        # max_train_per_session = int(140 / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        \n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0]),] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        \n",
    "        # Cross validation - choose best hyperparameters for each user\n",
    "        # Default hyperparameters (will be replaced by the best hyperparameters)\n",
    "        hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "        for hyper_pair in ParameterGrid(parameters):\n",
    "            # Use time series split cross-validation\n",
    "            cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "            avg_eer = []\n",
    "            for train, test in cv.split(X_train):\n",
    "                clf = OneClassSVM(\n",
    "                    kernel = hyper_pair['kernel'], \n",
    "                    nu=hyper_pair['nu'], \n",
    "                    degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "                    gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                clf.fit(scaler.fit_transform(X_train[train]))\n",
    "\n",
    "                # Predict\n",
    "                res = np.concatenate([\n",
    "                    clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "                        clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "                ])\n",
    "                # Build labels based on test data and result\n",
    "                y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "                \n",
    "                # Set inf and -inf predictions to a reasonable number\n",
    "                res[np.isneginf(res)] = -100000\n",
    "                res[np.isposinf(res)] = 100000\n",
    "                avg_eer.append(compute_eer(y_test, res))\n",
    "            hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "        \n",
    "        # Get the best hyperparams\n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "\n",
    "        # Normalize the data using StandardScaler - fit only on train data and use the same scaler for both train and test\n",
    "        scaler = StandardScaler()\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        # Predict\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        # Build labels based on the test data and result - user labels are set as 1, intruder labels are set as -1\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters), f)\n",
    "\n",
    "        # print(compute_eer(y_test, res))\n",
    "\n",
    "\n",
    "        # results[user_id]['eer'] = compute_eer(y_test, res)\n",
    "\n",
    "    def find_hyper(user_id, hyper_pair, users, valid_users, X_train, hyperparameters):\n",
    "        cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "        avg_eer = []\n",
    "        for train, test in cv.split(X_train):\n",
    "            clf = OneClassSVM(\n",
    "                kernel = hyper_pair['kernel'], \n",
    "                nu=hyper_pair['nu'], \n",
    "                degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "                gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "            scaler = StandardScaler()\n",
    "            clf.fit(scaler.fit_transform(X_train[train]))\n",
    "            res = np.concatenate([\n",
    "                clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "                    clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "            ])\n",
    "            y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "            \n",
    "            res[np.isneginf(res)] = -1000\n",
    "            res[np.isposinf(res)] = 1000\n",
    "            avg_eer.append(compute_eer(y_test, res))\n",
    "        hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "\n",
    "    # results = [{i: {}} for i in range(300)]\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # ERROR HERE\n",
    "    for user_id in list(set(valid_users).intersection([0,1] if DATASET == BRAINRUN else [])):\n",
    "        max_train_per_session = np.inf\n",
    "\n",
    "        # Uncomment this line to test the effect of different number of gestures\n",
    "        # max_train_per_session = int(140 / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        \n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0]),] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "\n",
    "        # Cross validation\n",
    "        # Users 0 and 1 of the BrainRun dataset contain significantly more data, and cross-validation is done multi-threaded for efficiency.\n",
    "        # For all other users cross-validation is done in a single thread, as the overhead is not justified.\n",
    "        threads = []\n",
    "        cid = 0\n",
    "        can_exit = False\n",
    "\n",
    "        hyper_grid = list(ParameterGrid(parameters))\n",
    "        manager = mp.Manager()\n",
    "        hyperparameters = manager.list()\n",
    "\n",
    "        pbar = tqdm(total=len(hyper_grid))\n",
    "        while not can_exit:\n",
    "            while len(threads) < N_THREADS and cid < len(hyper_grid):\n",
    "                thread = mp.Process(target=find_hyper, args=(user_id, hyper_grid[cid], users, valid_users, X_train, hyperparameters))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "                pbar.update(1)\n",
    "                cid += 1\n",
    "\n",
    "            for thread in threads:\n",
    "                if not thread.is_alive():\n",
    "                    thread.join()\n",
    "                    threads.remove(thread)\n",
    "\n",
    "            if(len(threads) == 0):\n",
    "                can_exit = True\n",
    "            time.sleep(1)\n",
    "        pbar.close()\n",
    "\n",
    "        # End hyperparameter search\n",
    "\n",
    "        # Fit and test the model (same process as before)\n",
    "        hyperparameters = list(hyperparameters)\n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters), f)\n",
    "\n",
    "    threads = []\n",
    "    cid = 0\n",
    "    can_exit = False\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # Same process of excluding the first two users for the BrainRun dataset\n",
    "    vu = list(set(valid_users).difference([0,1] if DATASET == BRAINRUN else []))\n",
    "    pbar = tqdm(total=len(vu))\n",
    "    while not can_exit:\n",
    "        while len(threads) < N_THREADS and cid < len(vu):\n",
    "            user_id = vu[cid]\n",
    "            thread = mp.Process(target=run_experiment_for_user, args=(vu[cid], users, valid_users, parameters))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "            cid += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                thread.join()\n",
    "                threads.remove(thread)\n",
    "\n",
    "        if(len(threads) == 0):\n",
    "            can_exit = True\n",
    "\n",
    "        time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments with the set parameters|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 17/41 [00:20<00:28,  1.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c39e502f4280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mparse_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvalid_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_users_over_gestures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8db56387df31>\u001b[0m in \u001b[0;36mparse_user\u001b[0;34m(user_id)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'devices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sessions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'devices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sessions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_to_datapoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'devices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sessions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Outlier detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalOutlierFactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontamination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb69b3c361e2>\u001b[0m in \u001b[0;36msession_to_datapoints\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mconsiders\u001b[0m \u001b[0meach\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     '''\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mfeaturized_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgesture_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gestures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     sliding_windows = (\n\u001b[1;32m    102\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb69b3c361e2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mconsiders\u001b[0m \u001b[0meach\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     '''\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mfeaturized_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgesture_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gestures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     sliding_windows = (\n\u001b[1;32m    102\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb69b3c361e2>\u001b[0m in \u001b[0;36mgesture_to_data\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Stroke duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mdelta_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_stop'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mextra_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Start and stop time are removed when sliding windows is called, and transformed in relative time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb69b3c361e2>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(data, delta_time)\u001b[0m\n\u001b[1;32m     43\u001b[0m     result.append(\n\u001b[1;32m     44\u001b[0m         np.max(\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         ))\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb69b3c361e2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m     result.append(\n\u001b[1;32m     44\u001b[0m         np.max(\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         ))\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/local/home/r96133vi/venv/lib64/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if DATASET == BRAINRUN:\n",
    "    with open(f'{DATA_PATH}/brainrun_full_not_parsed.pkl', 'rb') as f:\n",
    "        users = pickle.load(f)\n",
    "\n",
    "if DATASET == TOUCHALYTICS:\n",
    "    with open(f'{DATA_PATH}/touchalytics_full_not_parsed.pkl', 'rb') as f:\n",
    "        users = pickle.load(f)\n",
    "\n",
    "for user in tqdm(range(len(users))):\n",
    "    parse_user(user)\n",
    "\n",
    "valid_users = get_users_over_gestures(140)\n",
    "\n",
    "# Uncomment the following lines to see the effect of using less users\n",
    "# These experiments were performed multiple times, and the results were averaged\n",
    "# np.random.shuffle(valid_users)\n",
    "# valid_users = valid_users[:<MAX_USER_SIZE>]\n",
    "\n",
    "run_experiment_multithreaded(OUTPUT_DIR, DATASET, valid_users)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecc757a38b814f9d9a78e27a3f0556e4d5be26951c473c897a83b93183395c00"
  },
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
