{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN model 1\n",
        "\n",
        "In this CNN model, the data representation is similar to the one used in [this paper](https://ieeexplore.ieee.org/document/7458136), and in the HAR field using motion sensors. \n",
        "The first and last 5 points of a gesture are flattened and concatenated in a feature vector, to which time information is added. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A0v6Qaw9htTS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tqdm import tqdm\n",
        "\n",
        "import keras_tuner as kt\n",
        "from config import *\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters for the experiments\n",
        "Set the parameters with which the experiment will be run. Below is a description for each of them.\n",
        "* `DATASET`: Specify which dataset to be used. Only `BRAINRUN` is available for theses experiments The paths for the dataset are assumed to be at `./datasets`, and can be changed in the `config.py` file.\n",
        "* `MODELS_DIR`: Directory where to save trained models to be used in user identification.\n",
        "\n",
        "Here parameters for data cleanup and parsing are also set, although these are best left constant.\n",
        "* `MIN_SESSION_GESTURES`: Minimum number of gestures in a session to be considered. Left at 140 as it has shown to produce good results, and include a large number of users\n",
        "* `SCREENS`: The screens to use from the BrainRun dataset when performing the experiment (ignored for the Touchalytics dataset). The experiments were originally performed using either one or both of the screens *MathisisGame* or *FocusGame*, as they contain predominantly swipe data.\n",
        "* `WINDOW_SIZE`: Needs to be the size of the CNN input layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RvbgC097htTZ"
      },
      "outputs": [],
      "source": [
        "# Training only performed on the BrainRun dataset\n",
        "DATASET = BRAINRUN\n",
        "WINDOW_SIZE = 11\n",
        "MODELS_DIR = f'models/model_2_ws_{WINDOW_SIZE}'\n",
        "\n",
        "MIN_SESSION_GESTURES = 140\n",
        "SCREENS = ['MathisisGame', 'FocusGame']\n",
        "\n",
        "# Size of the input image (assume square). All experiments used an image of size 32x32.\n",
        "IMAGE_SIZE = 32\n",
        "# Size of the canvas to draw the gesture. All experiments used a canvas of size 128x128, and \n",
        "# the resulting images were downsampled to 32x32.\n",
        "CANVAS_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility methods for parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cD2UXrzBhtTe"
      },
      "outputs": [],
      "source": [
        "def draw_line(x0, y0, x1, y1, img_size, img, vel = 1):\n",
        "    '''\n",
        "    Draws a line starting from (x0, y0) and ending at (x1, y1) on the image `img`, with the intensity\n",
        "    set by `vel`.\n",
        "    '''\n",
        "    rr,cc,val = line_aa(int(x0 * img_size), int(y0 * img_size), int(x1 * img_size), int(y1 * img_size))\n",
        "    if x0 > 1 or y0 > 1 or x1 > 1 or y1 > 1:\n",
        "        print(x0, y0, x1, y1)\n",
        "    img[rr,cc] = val * vel\n",
        "\n",
        "def points_to_image(points):\n",
        "    '''\n",
        "    Converts a list of points ((x,y) pairs) into an image of size `IMAGE_SIZE` x `IMAGE_SIZE`.\n",
        "    Encodes velocities between each pair of points as intensity in the image.\n",
        "    '''\n",
        "    init = np.zeros((CANVAS_SIZE, CANVAS_SIZE))\n",
        "\n",
        "    # Get a list of velocities between pairwise points\n",
        "    velocities = [np.linalg.norm(p1 - p2) for p1, p2 in zip(points[:-1], points[1:])]\n",
        "    velocities = (velocities - np.min(velocities)) / (np.ptp(velocities) or 1)\n",
        "\n",
        "    # Draw each line segment\n",
        "    x0, y0 = points[0][1], points[0][0]\n",
        "    for datapoint, velocity in zip(points[1:], velocities):\n",
        "        draw_line(x0, y0, datapoint[1], datapoint[0], 128 - 1, init, vel = velocity)\n",
        "        x0, y0 = datapoint[1], datapoint[0]\n",
        "\n",
        "    # Resize the image to the desired size\n",
        "    res = init\n",
        "    res = cv2.resize(init, dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\n",
        "    mean, std = res.mean(), res.std()\n",
        "    res = (res - mean) / (std or 1)\n",
        "    res = res.reshape(IMAGE_SIZE, IMAGE_SIZE, 1)\n",
        "    return res\n",
        "\n",
        "def gesture_to_image(c):\n",
        "    '''\n",
        "    Convert a gesture into an image representing the path of the gesture.\n",
        "    '''\n",
        "    clip = lambda x: np.clip(x, 0, 1)\n",
        "    img_data = c['data']\n",
        "    \n",
        "    points = np.array([[clip(img_data[0]['x0']), clip(img_data[0]['y0'])]] + [[clip(pt['moveX']), clip(pt['moveY'])] for pt in img_data])\n",
        "    return points_to_image(points) \n",
        "\n",
        "def window_to_datapoint(window):\n",
        "    '''\n",
        "    Sum WINDOW_SIZE windows into a single image.\n",
        "    '''\n",
        "    return np.sum(window, axis = 0)\n",
        "    \n",
        "def session_to_datapoints(s):\n",
        "    '''\n",
        "    Convert a session to a series of datapoints, each representing a window of length WINDOW_SIZE.\n",
        "    '''\n",
        "    featurized_session = np.array([gesture_to_image(x) for x in s['gestures']])\n",
        "    sliding_windows = (\n",
        "        np.expand_dims(np.arange(WINDOW_SIZE), 0) +\n",
        "        np.expand_dims(np.arange(len(featurized_session) - WINDOW_SIZE), 0).T\n",
        "    )\n",
        "\n",
        "    return np.array([window_to_datapoint(window) for window in featurized_session[sliding_windows]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods for filtering and parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b0jYM0CLhtTi"
      },
      "outputs": [],
      "source": [
        "# Limit the size of the first and second user to the maximum number of  gestures in\n",
        "# remaining dataset to avoid imbalance\n",
        "LIMIT_GESTURES_PER_USER = 6825\n",
        "\n",
        "def prefilter_session(s):\n",
        "    '''\n",
        "    Filters the session, orders gestures chronologically and removes gestures that are outliers or from different screens\n",
        "    '''\n",
        "    s['gestures'].sort(key = lambda x: x['t_start'])\n",
        "    s['gestures'] = [x for x in s['gestures'] \n",
        "        if x['t_stop'] - x['t_start'] > 70 and x['t_stop'] - x['t_start'] < (1000 if DATASET == BRAINRUN else 2000) and \n",
        "        ((x['screen'].split(' ')[0] in SCREENS and x['type'] == 'swipe') if DATASET == BRAINRUN else True)]\n",
        "\n",
        "def parse_user(user_id):\n",
        "    '''\n",
        "    Parses all the sessions for a user with the given id. Deletes sessions that are too short after filtering them.\n",
        "    '''\n",
        "    i = 0\n",
        "\n",
        "    while i < len(users[user_id]['devices'][0]['sessions']):\n",
        "        prefilter_session(users[user_id]['devices'][0]['sessions'][i])\n",
        "\n",
        "        if len(users[user_id]['devices'][0]['sessions'][i]['gestures']) < MIN_SESSION_GESTURES:\n",
        "            del users[user_id]['devices'][0]['sessions'][i]\n",
        "        else:\n",
        "            users[user_id]['devices'][0]['sessions'][i] = session_to_datapoints(users[user_id]['devices'][0]['sessions'][i])\n",
        "            i += 1\n",
        "\n",
        "    # Limit gestures for the first and second user\n",
        "    # Select gestures stratified in order to include as evenly as possible from all sessions\n",
        "    inx_len = np.argsort([len(x) for x in users[user_id]['devices'][0]['sessions']])\n",
        "    sessions_remaining = len(users[user_id]['devices'][0]['sessions'])\n",
        "    gestures_remaining = LIMIT_GESTURES_PER_USER\n",
        "    for i in inx_len:\n",
        "        gestures_this_session = int(gestures_remaining / sessions_remaining)\n",
        "        gestures_remaining -= gestures_this_session\n",
        "        users[user_id]['devices'][0]['sessions'][i] = users[user_id]['devices'][0]['sessions'][i][:gestures_this_session]\n",
        "        sessions_remaining -= 1\n",
        "\n",
        "def get_users_over_gestures(number_of_gestures = 140):\n",
        "    '''\n",
        "    Returns an array with the indices of all users with more than number_of_gestures gestures.\n",
        "    '''\n",
        "    uc = np.zeros((len(users), ))\n",
        "    for i in range(len(users)):\n",
        "        uc[i] = 0\n",
        "        for session in users[i]['devices'][0]['sessions']:\n",
        "            uc[i] += session.shape[0]\n",
        "\n",
        "    return np.where(uc > number_of_gestures)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Methods used for splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bp2bu9-EhtTm"
      },
      "outputs": [],
      "source": [
        "def get_train_indices(size, test_size = 0.2, val_size = 0.1, gap = WINDOW_SIZE, max_size = 50000):\n",
        "    size = min(size, max_size)\n",
        "    middle = int(size * (1 - test_size - val_size))\n",
        "    return np.arange(middle - gap)\n",
        "\n",
        "def get_val_indices(size, test_size = 0.2, val_size = 0.1, gap = WINDOW_SIZE, max_size = 50000):\n",
        "    size = min(size, max_size)\n",
        "    start = int(size * (1 - test_size - val_size))\n",
        "    end = int(size * (1 - test_size))\n",
        "    return np.arange(start, end - gap)\n",
        "\n",
        "def get_test_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 50000):\n",
        "    size = min(size, max_size)\n",
        "    middle = int(size * (1 - test_size))\n",
        "    return np.arange(middle, size)\n",
        "\n",
        "def get_train_val_test_data_for_users(valid_users):\n",
        "    '''\n",
        "    Splits the data for a list of users into train, val and test, stratified for each session and merginf the session data in the process.\n",
        "    '''\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = [], [], [], [], [], []\n",
        "    for user_id in valid_users:\n",
        "        temp_x_train = np.concatenate([session[get_train_indices(session.shape[0])] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
        "        temp_x_val = np.concatenate([session[get_val_indices(session.shape[0])] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
        "        temp_x_test = np.concatenate([session[get_test_indices(session.shape[0])] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
        "        X_train.append(temp_x_train)\n",
        "        X_val.append(temp_x_val)\n",
        "        X_test.append(temp_x_test)\n",
        "        y_train.append(np.zeros(temp_x_train.shape[0]) + user_id)\n",
        "        y_val.append(np.zeros(temp_x_val.shape[0]) + user_id)\n",
        "        y_test.append(np.zeros(temp_x_test.shape[0]) + user_id)\n",
        "      \n",
        "    X_train = np.concatenate(X_train)\n",
        "    X_val = np.concatenate(X_val)\n",
        "    X_test = np.concatenate(X_test)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    y_val = np.concatenate(y_val)\n",
        "    y_test = np.concatenate(y_test)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(y_train)\n",
        "    y_train = encoder.transform(y_train)\n",
        "    y_val = encoder.transform(y_val)\n",
        "    y_test = encoder.transform(y_test)\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train)\n",
        "    y_val = np_utils.to_categorical(y_val)\n",
        "    y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Methods for building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Kkr727D3htTp"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "  '''\n",
        "  Build the model with the best hyperparameters (dense size 128).\n",
        "  '''\n",
        "  dropout_rate = 0.3\n",
        "\n",
        "  input_shape = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "\n",
        "  cnn = layers.Conv2D(128, (5, 5), activation='linear')(input_shape)\n",
        "  cnn = layers.BatchNormalization()(cnn)\n",
        "  cnn = layers.ReLU()(cnn)\n",
        "  cnn = layers.MaxPool2D()(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "\n",
        "  cnn = layers.Conv2D(32, (3, 3), activation='relu')(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "  cnn = layers.MaxPool2D()(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "\n",
        "  cnn = layers.Conv2D(128, (3, 3), activation='relu')(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "  cnn = layers.MaxPool2D()(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "\n",
        "  cnn = layers.Flatten()(cnn)\n",
        "\n",
        "  dense = layers.Dense(256, activation='relu')(cnn)\n",
        "  dense = layers.Dense(128, activation='relu')(dense)\n",
        "  dense = layers.Dense(y_train.shape[1])(dense)\n",
        "\n",
        "  dense = layers.Softmax()(dense)\n",
        "\n",
        "  model = models.Model(input_shape, dense)\n",
        "\n",
        "  lr = 0.001\n",
        "  decay = 0.8\n",
        "  epsilon = 1e-7\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr, beta_1=decay, epsilon=epsilon),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
        "  return model\n",
        "\n",
        "# Monitor training time\n",
        "class MonitorTime(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super(MonitorTime, self).__init__()\n",
        "    \n",
        "    def on_train_begin(self, *args):\n",
        "        self.start_time= time.time()\n",
        "        \n",
        "    def on_train_end(self, *args):\n",
        "        stop_time=time.time()\n",
        "        duration = stop_time- self.start_time             \n",
        "        print(duration) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gVzzUUG0vnh1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 225/225 [02:55<00:00,  1.28it/s]\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "with open(f'{DATA_PATH}/brainrun_full_not_parsed.pkl', 'rb') as f:\n",
        "    users = pickle.load(f)\n",
        "\n",
        "for user in tqdm(range(len(users))):\n",
        "    parse_user(user)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# User identification experiment\n",
        "Run the experiment to identify the users using the CNN model with the best parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nW-ybrthtTr"
      },
      "outputs": [],
      "source": [
        "valid_users = get_users_over_gestures(140)\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data_for_users(valid_users)\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# Stop early to avoid overfit\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size = 256, validation_data = (X_val, y_val), callbacks=[stop_early, MonitorTime()])\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in3t-7_9lWfA"
      },
      "source": [
        "# Hyperparameter tuning\n",
        "Select the best hyperparameters for a certain model architecture.\n",
        "Only the best model hyperparameter tuning is presented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13btCnhbhtTq"
      },
      "outputs": [],
      "source": [
        "def build_model_with_hp(hp):\n",
        "  '''\n",
        "  Choose best hyperparameters for the model (128 dense layer)\n",
        "  '''\n",
        "\n",
        "  dropout_rate = hp.Choice('dropout', [0.0, 0.1, 0.2, 0.3, 0.4])\n",
        "\n",
        "  input_shape = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "\n",
        "  filters_layer_1 = hp.Choice('filters_1', [32, 64, 128])\n",
        "  kernel_size_layer_1 = hp.Choice('kernel_size_1', [3, 5])\n",
        "  cnn = layers.Conv2D(filters_layer_1, (kernel_size_layer_1, kernel_size_layer_1), activation='linear')(input_shape)\n",
        "  cnn = layers.BatchNormalization()(cnn)\n",
        "  cnn = layers.ReLU()(cnn)\n",
        "  cnn = layers.MaxPool2D()(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "\n",
        "  filters_layer_2 = hp.Choice('filters_2', [32, 64, 128])\n",
        "  cnn = layers.Conv2D(filters_layer_2, (3, 3), activation='relu')(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "  cnn = layers.MaxPool2D()(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "\n",
        "  filters_layer_3 = hp.Choice('filters_3', [32, 64, 128])\n",
        "  cnn = layers.Conv2D(filters_layer_3, (3, 3), activation='relu')(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "  cnn = layers.MaxPool2D()(cnn)\n",
        "  cnn = layers.Dropout(dropout_rate)(cnn)\n",
        "\n",
        "  cnn = layers.Flatten()(cnn)\n",
        "\n",
        "  dense_size = hp.Choice('dense_size', [128, 256, 512, 1024])\n",
        "  dense = layers.Dense(dense_size, activation='relu')(cnn)\n",
        "  dense = layers.Dense(128, activation='relu')(dense)\n",
        "  dense = layers.Dense(y_train.shape[1])(dense)\n",
        "\n",
        "  dense = layers.Softmax()(dense)\n",
        "\n",
        "  model = models.Model(input_shape, dense)\n",
        "\n",
        "  lr = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
        "  decay = hp.Choice('decay', values=[0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\n",
        "  epsilon = hp.Choice('epsilon', values=[1e-5, 1e-6, 1e-7, 1e-8, 1e-9])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr, beta_1=decay, epsilon=epsilon),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
        "  return model\n",
        "\n",
        "tuner = kt.Hyperband(build_model_with_hp,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=50,\n",
        "                     factor=2,\n",
        "                     directory='model_hypertune',\n",
        "                     project_name='model_2')\n",
        "\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=50, batch_size = 256, validation_data = (X_val, y_val), callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtzb0b9dq5Ob"
      },
      "source": [
        "# Train models for user authentication\n",
        "\n",
        "Perform 10 iteration, randomly selecting 90% of the users for training and the rest for  testing. Save both the users, models and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYjVa-yxrH9X"
      },
      "outputs": [],
      "source": [
        "for iteration in range(10):\n",
        "  all_users = get_users_over_gestures(140)\n",
        "  np.random.shuffle(all_users)\n",
        "  train_users = all_users[:int(0.9 * len(all_users))]\n",
        "  test_users = all_users[int(0.9 * len(all_users)):]\n",
        "\n",
        "  X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data_for_users(valid_users)\n",
        "\n",
        "  model = build_model()\n",
        "\n",
        "  # Stop early to avoid overfit\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "  history = model.fit(X_train, y_train, epochs=50, batch_size = 256, validation_data = (X_val, y_val), callbacks=[stop_early])\n",
        "  test_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "  model.save(f'{MODELS_DIR}/models/simple_cnn_128_embedding_{iteration}.h5')\n",
        "  with open(f'{MODELS_DIR}/results/iteration_{iteration}.pkl', 'wb') as f:\n",
        "      pickle.dump([train_users, test_users, history.history, test_results], f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CNN simple feed gesture points_no_hidden_layer.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ecc757a38b814f9d9a78e27a3f0556e4d5be26951c473c897a83b93183395c00"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
