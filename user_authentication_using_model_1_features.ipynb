{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One class SVM model using automatically extracted features - Model 1\n",
    "\n",
    "This notebook contains the code to run experiments for intruder detection using the first CNN model (Model 1) in order to automatically\n",
    "extract features from gestures. The model should extract both single-gesture features, as well as inter-gesture features relating to the\n",
    "time difference in a sliding window. \n",
    "\n",
    "The experiments are run similar to the ones for the 1-class SVM with manually engineered features. However, these notebook requries a `models` folder\n",
    "containig the trained models, and users used for training the models (in order to be excluded). Links to these resources can be found in the `README.md` file. \n",
    "\n",
    "Experiments run multithreaded and save the output in the `OUTPUT_DIR` folder. Set the `N_THREADS` variable to the number of threads you want to use (recommended 4-8 threads on a normal machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for the experiments\n",
    "Set the parameters with which the experiment will be run. Below is a description for each of them.\n",
    "* `DATASET`: Specify which dataset to be used. Choose between BRAINRUN and TOUCHALYTICS. The paths for the dataset are assumed to be at `./datasets`, and can be changed in the `config.py` file.\n",
    "* `N_THREADS`: Number of threads to use for the experiment.\n",
    "* `OUTPUT_DIR`: Directory to which the results will be written.\n",
    "* `MODELS_DIR`: Directory containing the trained models.\n",
    "\n",
    "Here we also set the parameters for data cleanup and parsing, although these are best left constant.\n",
    "* `MIN_SESSION_GESTURES`: Minimum number of gestures in a session to be considered. Left at 140 as it has shown to produce good results, and include a large number of users\n",
    "* `SCREENS`: The screens to use from the BrainRun dataset when performing the experiment (ignored for the Touchalytics dataset). The experiments were originally performed using either one or both of the screens *MathisisGame* or *FocusGame*, as they contain predominantly swipe data.\n",
    "* `WINDOW_SIZE`: Needs to be the same as the size of the input to the model (constant 11 throught the experiment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = TOUCHALYTICS # Choose between BRAINRUN and TOUCHALYTICS\n",
    "N_THREADS = 32\n",
    "OUTPUT_DIR = 'test_results2/'\n",
    "MODELS_DIR = 'models/model_1'\n",
    "\n",
    "MIN_SESSION_GESTURES = 140\n",
    "SCREENS = ['MathisisGame', 'FocusGame']\n",
    "\n",
    "# WARNING: Window size has to be set to the size of the model input layer\n",
    "WINDOW_SIZE = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods for extracting features and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gesture_to_points(data):\n",
    "    '''\n",
    "    Convert a gesture to a list of the first 5 and last 5 points (flatenned)\n",
    "    '''\n",
    "    result = []\n",
    "    points = [(data[0]['x0'], data[0]['y0'])] + [(x['moveX'], x['moveY']) for x in data]\n",
    "    points = np.array(points)\n",
    "\n",
    "    res = np.nan_to_num(result)\n",
    "    first_three_points = points[:5].flatten() \n",
    "    last_three_points = points[-5:].flatten() \n",
    "    first_three_points.resize((10,))\n",
    "    last_three_points.resize((10,))\n",
    "\n",
    "    return np.concatenate([res, first_three_points, last_three_points])\n",
    "    \n",
    "def gesture_to_data(c):\n",
    "    '''\n",
    "    Convert a gesture to a list of points, including start and stop time of the gesture. These will be\n",
    "    removed later\n",
    "    '''\n",
    "    # Swipe duration\n",
    "    delta_time = (c['t_stop'] - c['t_start']) / 1000\n",
    "    extra_features = gesture_to_points(c['data'])\n",
    "\n",
    "    # Start and stop time removed when sliding window is used\n",
    "    return np.concatenate([[c['t_start'], c['t_stop'], delta_time], extra_features])\n",
    "\n",
    "def window_to_datapoint(window):\n",
    "    '''\n",
    "    Remove the start and stop time from a window and add inter-gesture features\n",
    "    '''\n",
    "    return np.concatenate([window[:, 2:], # Exclude start and stop time\n",
    "        np.concatenate([[0], (window[1:, 0] - window[0, 1]).flatten() / 1000]).reshape(window.shape[0],1), # Window start - initial point stop\n",
    "        np.concatenate([[0], (window[1:, 0] - window[:-1, 1]).flatten() / 1000]).reshape(window.shape[0],1)], axis = 1).reshape(window.shape[0], window.shape[1], 1) # Window start - previous window stop\n",
    "\n",
    "def session_to_datapoints(s):\n",
    "    '''\n",
    "    Convert a session to a series of datapoints, representing the 128 feature embeddings of\n",
    "    the windows.\n",
    "    '''\n",
    "    featurized_session = np.array([gesture_to_data(x) for x in s['gestures']])\n",
    "    sliding_windows = (\n",
    "        np.expand_dims(np.arange(WINDOW_SIZE), 0) +\n",
    "        np.expand_dims(np.arange(len(featurized_session) - WINDOW_SIZE), 0).T\n",
    "    )\n",
    "\n",
    "    temp = np.array([window_to_datapoint(window) for window in featurized_session[sliding_windows]])\n",
    "    return deep_model.predict(temp)\n",
    "\n",
    "\n",
    "def get_train_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = np.inf):\n",
    "    '''\n",
    "    Returns the train indices for a given session.\n",
    "    Leaves a space of `gap` between the train and test indices\n",
    "    '''\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    middle = min(middle, max_size)\n",
    "    return np.arange(middle - gap)\n",
    "\n",
    "def get_test_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = np.inf):\n",
    "    '''\n",
    "    Returns the test indices for a given session.\n",
    "    '''\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    return np.arange(middle, size)\n",
    "\n",
    "def get_intruder_size(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = np.inf):\n",
    "    '''\n",
    "    Returns validation and test indices for intruders. Also leaves a space of `gap` \n",
    "    between the validation and test indices.\n",
    "    '''\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    validation_middle = int(size * (1 - test_size / 2))\n",
    "    return np.arange(middle, validation_middle - gap), np.arange(validation_middle, size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for filtering and parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def prefilter_session(s):\n",
    "    '''\n",
    "    Filters the session, orders gestures chronologically and removes gestures that are outliers or from different screens\n",
    "    '''\n",
    "    s['gestures'].sort(key = lambda x: x['t_start'])\n",
    "    s['gestures'] = [x for x in s['gestures'] \n",
    "        if x['t_stop'] - x['t_start'] > 70 and x['t_stop'] - x['t_start'] < (1000 if DATASET == BRAINRUN else 2000) and \n",
    "        ((x['screen'].split(' ')[0] in SCREENS and x['type'] == 'swipe') if DATASET == BRAINRUN else True)]\n",
    "\n",
    "def parse_user(user_id):\n",
    "    '''\n",
    "    Parses all the sessions for a user with the given id. Deletes sessions that are too short after filtering them.\n",
    "    '''\n",
    "\n",
    "    i = 0\n",
    "    should_delete = False\n",
    "    while i < len(users[user_id]['devices'][0]['sessions']):\n",
    "        prefilter_session(users[user_id]['devices'][0]['sessions'][i])\n",
    "\n",
    "        if len(users[user_id]['devices'][0]['sessions'][i]['gestures']) < MIN_SESSION_GESTURES or should_delete:\n",
    "            del users[user_id]['devices'][0]['sessions'][i]\n",
    "        else:\n",
    "            users[user_id]['devices'][0]['sessions'][i] = session_to_datapoints(users[user_id]['devices'][0]['sessions'][i])\n",
    "            # Outlier detection\n",
    "            clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "            users[user_id]['devices'][0]['sessions'][i] = \\\n",
    "                users[user_id]['devices'][0]['sessions'][i][np.where(clf.fit_predict(users[user_id]['devices'][0]['sessions'][i]) == 1)]\n",
    "            i += 1\n",
    "\n",
    "def get_users_over_gestures(number_of_gestures = 140):\n",
    "    '''\n",
    "    Returns an array with the indices of all users with more than number_of_gestures gestures.\n",
    "    '''\n",
    "    uc = np.zeros((len(users), ))\n",
    "    for i in range(len(users)):\n",
    "        uc[i] = 0\n",
    "        for session in users[i]['devices'][0]['sessions']:\n",
    "            uc[i] += session.shape[0]\n",
    "\n",
    "    return np.where(uc > number_of_gestures)[0]\n",
    "\n",
    "def compute_eer(label, pred):\n",
    "    \"\"\"\n",
    "    Computes EER given a list of labels and predictions.\n",
    "\n",
    "    Code inspired by https://github.com/YuanGongND/python-compute-eer\n",
    "    \"\"\"\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    fpr, tpr, threshold = roc_curve(label, pred)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for multithreaded experiments\n",
    "\n",
    "Since the datasets are large and a iteration is done for each users, the experiments take a significant amount of time to run on a normal machine. Multiprocessing was used to run the experiments in parallel on a powerful machine, using 32 cores (this reduces the time to about an hour). Below is the code used to run the experiments using different processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter space (for hyperparameter tuning)\n",
    "parameters = [{\n",
    "    'kernel': ['rbf'], 'gamma': [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001], 'nu': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],},\n",
    "    ]\n",
    "\n",
    "def run_experiment_multithreaded(output_path, dataset, valid_users):\n",
    "    '''\n",
    "    Runs experiment with the data provided in the `users` variable, and outputs the results to `output_path`.\n",
    "    The first and second userss of the BrainRun dataset contain significantly more data than the others, so filter \n",
    "    the hyperparameter selection is run multithreaded for them.\n",
    "    '''\n",
    "    FILE_PATH = f'{output_path}/{dataset}'\n",
    "\n",
    "    def run_experiment_for_user(user_id, users, valid_users, parameters):        \n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0],)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0]),] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        \n",
    "        # Cross validation - choose best hyperparameters for each user\n",
    "        # Default hyperparameters (will be replaced by the best hyperparameters)\n",
    "        hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "        for hyper_pair in ParameterGrid(parameters):\n",
    "            # Use time series split cross-validation\n",
    "            cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "            avg_eer = []\n",
    "            for train, test in cv.split(X_train):\n",
    "                clf = OneClassSVM(\n",
    "                    kernel = hyper_pair['kernel'], \n",
    "                    nu=hyper_pair['nu'], \n",
    "                    degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "                    gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                clf.fit(scaler.fit_transform(X_train[train]))\n",
    "\n",
    "                # Predict\n",
    "                res = np.concatenate([\n",
    "                    clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "                        clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "                ])\n",
    "                # Build labels based on test data and result\n",
    "                y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "                \n",
    "                # Set inf and -inf predictions to a reasonable number\n",
    "                res[np.isneginf(res)] = -100000\n",
    "                res[np.isposinf(res)] = 100000\n",
    "                avg_eer.append(compute_eer(y_test, res))\n",
    "            hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "        \n",
    "        # Get the best hyperparams\n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "\n",
    "        # Normalize the data using StandardScaler - fit only on train data and use the same scaler for both train and test\n",
    "        scaler = StandardScaler()\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        # Predict\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        # Build labels based on the test data and result - user labels are set as 1, intruder labels are set as -1\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters), f)\n",
    "\n",
    "    def find_hyper(user_id, hyper_pair, users, valid_users, X_train, hyperparameters):\n",
    "        '''\n",
    "        Finds the best hyperparmaters for a specific user.\n",
    "        '''\n",
    "        cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "        avg_eer = []\n",
    "        for train, test in cv.split(X_train):\n",
    "            clf = OneClassSVM(\n",
    "                kernel = hyper_pair['kernel'], \n",
    "                nu=hyper_pair['nu'], \n",
    "                degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "                gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "            scaler = StandardScaler()\n",
    "            clf.fit(scaler.fit_transform(X_train[train]))\n",
    "            res = np.concatenate([\n",
    "                clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "                    clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "            ])\n",
    "            y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "            \n",
    "            res[np.isneginf(res)] = -1000\n",
    "            res[np.isposinf(res)] = 1000\n",
    "            avg_eer.append(compute_eer(y_test, res))\n",
    "        hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    for user_id in list(set(valid_users).intersection([0,1] if DATASET == BRAINRUN else [])):\n",
    "        max_train_per_session = np.inf\n",
    "\n",
    "        # Uncomment this line to test the effect of different number of gestures\n",
    "        # max_train_per_session = int(140 / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        \n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0]),] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "\n",
    "        # Cross validation\n",
    "        # Users 0 and 1 of the BrainRun dataset contain significantly more data, and cross-validation is done multi-threaded for efficiency.\n",
    "        # For all other users cross-validation is done in a single thread, as the overhead is not justified.\n",
    "        threads = []\n",
    "        cid = 0\n",
    "        can_exit = False\n",
    "\n",
    "        hyper_grid = list(ParameterGrid(parameters))\n",
    "        manager = mp.Manager()\n",
    "        hyperparameters = manager.list()\n",
    "\n",
    "        pbar = tqdm(total=len(hyper_grid))\n",
    "        while not can_exit:\n",
    "            while len(threads) < N_THREADS and cid < len(hyper_grid):\n",
    "                thread = mp.Process(target=find_hyper, args=(user_id, hyper_grid[cid], users, valid_users, X_train, hyperparameters))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "                pbar.update(1)\n",
    "                cid += 1\n",
    "\n",
    "            for thread in threads:\n",
    "                if not thread.is_alive():\n",
    "                    thread.join()\n",
    "                    threads.remove(thread)\n",
    "\n",
    "            if(len(threads) == 0):\n",
    "                can_exit = True\n",
    "            time.sleep(1)\n",
    "        pbar.close()\n",
    "\n",
    "        # End hyperparameter search\n",
    "\n",
    "        # Fit and test the model (same process as before)\n",
    "        hyperparameters = list(hyperparameters)\n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters), f)\n",
    "\n",
    "    threads = []\n",
    "    cid = 0\n",
    "    can_exit = False\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # Same process of excluding the first two users for the BrainRun dataset\n",
    "    vu = list(set(valid_users).difference([0,1] if DATASET == BRAINRUN else []))\n",
    "    pbar = tqdm(total=len(vu))\n",
    "    while not can_exit:\n",
    "        while len(threads) < N_THREADS and cid < len(vu):\n",
    "            user_id = vu[cid]\n",
    "            thread = mp.Process(target=run_experiment_for_user, args=(vu[cid], users, valid_users, parameters))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "            cid += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                thread.join()\n",
    "                threads.remove(thread)\n",
    "\n",
    "        if(len(threads) == 0):\n",
    "            can_exit = True\n",
    "\n",
    "        time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 10 iterations, using the users the model was not trained on each time\n",
    "for iteration in range(10):\n",
    "    with open(f'{MODELS_DIR}/results/iteration_{iteration}.pkl', 'rb') as f:\n",
    "        u_training, u_testing, *_ = pickle.load(f)\n",
    "\n",
    "    # Only u_testing will be used \n",
    "    USERS_USED_FOR_TRAINING_FEAT_EXTRACTOR = u_training\n",
    "    USERS_USED_FOR_TESTING_GENERALIZATION = u_testing\n",
    "\n",
    "    # Load the model and remove the last layers, until the embedding layer is reached\n",
    "    deep_model = models.load_model(f'{MODELS_DIR}/models/simple_cnn_128_embedding_{iteration}.h5')\n",
    "    deep_model = models.Model(inputs = deep_model.input, outputs = deep_model.layers[-3].output)\n",
    "\n",
    "    # Load data\n",
    "    if DATASET == BRAINRUN:\n",
    "        with open('brainrun_full_not_parsed.pkl', 'rb') as f:\n",
    "            users = pickle.load(f)\n",
    "            valid_users = []\n",
    "            # Keep only the users on which the model wasn't trained on\n",
    "            for i, user in enumerate(users):\n",
    "                if i in USERS_USED_FOR_TESTING_GENERALIZATION:\n",
    "                    valid_users.append(user)\n",
    "\n",
    "            users = valid_users\n",
    "    if DATASET == TOUCHALYTICS:\n",
    "        with open('touchalytics_full_not_parsed.pkl', 'rb') as f:\n",
    "            users = pickle.load(f)\n",
    "\n",
    "    # Parse data\n",
    "    for user in tqdm(range(len(users))):\n",
    "        parse_user(user)\n",
    "\n",
    "    valid_users = get_users_over_gestures(140)\n",
    "\n",
    "    run_experiment_multithreaded(OUTPUT_DIR, iteration, valid_users)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecc757a38b814f9d9a78e27a3f0556e4d5be26951c473c897a83b93183395c00"
  },
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
