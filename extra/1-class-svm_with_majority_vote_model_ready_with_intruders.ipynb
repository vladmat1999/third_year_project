{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.display import JSON\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow import keras\n",
    "import pydot\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.draw import line_aa\n",
    "\n",
    "from IPython.display import Image \n",
    "\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_USERS = 10000\n",
    "MAX_SESSIONS_PER_USER = 10\n",
    "MIN_GESTURES_PER_SESSION = 100\n",
    "SOME_MAX_USERS = 2000\n",
    "MIN_GESTURES = 400\n",
    "NUM_PATHS = 10\n",
    "GESTURES = ['swipe']\n",
    "WINDOW_SIZE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (20, 92, 1)\n",
    "USER_DERIVED_FEATURES = True\n",
    "TOUCHALYTICS = 'touchalytics'\n",
    "BRAINRUN = 'brainrun'\n",
    "DATASET = BRAINRUN\n",
    "USE_PRESSURE_AREA = False\n",
    "\n",
    "def configure_cases(case_number): \n",
    "    global INPUT_SHAPE, USER_DERIVED_FEATURES\n",
    "\n",
    "    if case_number == 1:\n",
    "        INPUT_SHAPE = (20, 92, 1)\n",
    "        USER_DERIVED_FEATURES = True\n",
    "    elif case_number == 2:\n",
    "        INPUT_SHAPE = (20, 81, 1)\n",
    "        USER_DERIVED_FEATURES = False\n",
    "\n",
    "configure_cases(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pj(json):\n",
    "    print(dumps(json, indent = 4))\n",
    "\n",
    "def data_to_array(data):\n",
    "    return np.array([\n",
    "        data[k] for k in sorted(data.keys())\n",
    "    ])\n",
    "\n",
    "has_input = False\n",
    "\n",
    "def extract_features(data, delta_time, use_area_pressure = USE_PRESSURE_AREA):\n",
    "    result = []\n",
    "    points = [(data[0]['x0'], data[0]['y0'])] + [(x['moveX'], x['moveY']) for x in data]\n",
    "    points = np.array(points)\n",
    "\n",
    "    res = np.nan_to_num(result)\n",
    "    first_three_points = points[:5].flatten() \n",
    "    last_three_points = points[-5:].flatten() \n",
    "    first_three_points.resize((10,))\n",
    "    last_three_points.resize((10,))\n",
    "\n",
    "    return np.concatenate([res, first_three_points, last_three_points])\n",
    "    \n",
    "def gesture_to_data(c, use_extra_features = True):\n",
    "    delta_time = (c['t_stop'] - c['t_start']) / 1000\n",
    "    extra_features = extract_features(c['data'], delta_time) if use_extra_features else []\n",
    "    return np.concatenate([[c['t_start'], c['t_stop'], delta_time], extra_features])\n",
    "\n",
    "def window_to_datapoint(window):\n",
    "    # TODO CHECK AXIS\n",
    "    return np.concatenate([window[:, 2:],  # Exclude start and stop time\n",
    "    np.concatenate([[0], (window[1:, 0] - window[1:, 0]).flatten() / 1000]).reshape(window.shape[0],1), # Window start - initial point stop\n",
    "np.concatenate([[0], (window[1:, 0] - window[1:, 0]).flatten() / 1000]).reshape(window.shape[0],1)], axis = 1).reshape(window.shape[0], window.shape[1], 1)\n",
    "\n",
    "def session_to_datapoints(s):\n",
    "    featurized_session = np.array([gesture_to_data(x) for x in s['gestures']])\n",
    "    sliding_windows = (\n",
    "        np.expand_dims(np.arange(WINDOW_SIZE), 0) +\n",
    "        np.expand_dims(np.arange(len(featurized_session) - WINDOW_SIZE), 0).T\n",
    "    )\n",
    "\n",
    "    temp = np.array([window_to_datapoint(window) for window in featurized_session[sliding_windows]])\n",
    "    return deep_model.predict(temp)\n",
    "\n",
    "def session_to_dp_with_intruders(session, test_indices, n_intruders, intruders):\n",
    "    test_data = np.array([gesture_to_data(x) for x in session['gestures']])[test_indices]\n",
    "\n",
    "    sliding_windows = (\n",
    "        np.expand_dims(np.arange(WINDOW_SIZE), 0) +\n",
    "        np.expand_dims(np.arange(len(test_data) - WINDOW_SIZE), 0).T\n",
    "    )\n",
    "\n",
    "    windows = test_data[sliding_windows]\n",
    "\n",
    "    intruders = np.array([gesture_to_data(x) for x in intruders])\n",
    "    intruders = np.resize(intruders, (windows.shape[0], n_intruders, windows.shape[2]))\n",
    "\n",
    "    if n_intruders != 0:\n",
    "        windows[:, -n_intruders:, :] = intruders\n",
    "    temp = np.array([window_to_datapoint(window) for window in windows])\n",
    "\n",
    "    return deep_model.predict(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([[1,2],[3,4],[5,6],[7,8],[9,10]])\n",
    "\n",
    "sliding_windows = (\n",
    "        np.expand_dims(np.arange(3), 0) +\n",
    "        np.expand_dims(np.arange(len(test_data) - 3), 0).T\n",
    "    )\n",
    "\n",
    "windows = test_data[sliding_windows]\n",
    "print(windows)\n",
    "intr = np.array([[-1,-2],[-3,-4],[-5,-6],[-7,-8],[-9,-10]])\n",
    "n_intr = 1\n",
    "intruders = np.resize(intr, (windows.shape[0], n_intr, windows.shape[2]))\n",
    "\n",
    "print(intruders)\n",
    "\n",
    "windows[:, -n_intr:, :] = intruders\n",
    "print(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "LIMIT_GESTURES_PER_USER = 4000\n",
    "SESSION_GESTURES_MORE_THAN = 140\n",
    "good_screens = ['MathisisGame', 'FocusGame']\n",
    "\n",
    "def prefilter_session(s):\n",
    "    s['gestures'].sort(key = lambda x: x['t_start'])\n",
    "    s['gestures'] = [x for x in s['gestures'] \n",
    "        if x['t_stop'] - x['t_start'] > 70 and x['t_stop'] - x['t_start'] < (1000 if DATASET == BRAINRUN else 2000) and \n",
    "        ((x['screen'].split(' ')[0] in good_screens and x['type'] == 'swipe') if DATASET == BRAINRUN else True)]\n",
    "\n",
    "def parse_user(user_id):\n",
    "    i = 0\n",
    "\n",
    "    total = 0\n",
    "    should_delete = False\n",
    "    while i < len(users[user_id]['devices'][0]['sessions']):\n",
    "        prefilter_session(users[user_id]['devices'][0]['sessions'][i])\n",
    "\n",
    "        if len(users[user_id]['devices'][0]['sessions'][i]['gestures']) < SESSION_GESTURES_MORE_THAN or should_delete:\n",
    "            del users[user_id]['devices'][0]['sessions'][i]\n",
    "        else:\n",
    "            users[user_id]['devices'][0]['sessions'][i] = session_to_datapoints(users[user_id]['devices'][0]['sessions'][i])\n",
    "            total += len(users[user_id]['devices'][0]['sessions'][i])\n",
    "            i += 1\n",
    "\n",
    "def parse_user_dont_convert(user_id):\n",
    "    i = 0\n",
    "    total = 0\n",
    "    should_delete = False\n",
    "    while i < len(temp_users[user_id]['devices'][0]['sessions']):\n",
    "        prefilter_session(temp_users[user_id]['devices'][0]['sessions'][i])\n",
    "\n",
    "        if len(temp_users[user_id]['devices'][0]['sessions'][i]['gestures']) < SESSION_GESTURES_MORE_THAN or should_delete:\n",
    "            del temp_users[user_id]['devices'][0]['sessions'][i]\n",
    "        else:\n",
    "            total += len(temp_users[user_id]['devices'][0]['sessions'][i])\n",
    "            i += 1\n",
    "\n",
    "def get_users_over_gestures(number_of_gestures = 600):\n",
    "    uc = np.zeros((len(users), ))\n",
    "    for i in range(len(users)):\n",
    "        uc[i] = 0\n",
    "        for session in users[i]['devices'][0]['sessions']:\n",
    "            uc[i] += session.shape[0]\n",
    "\n",
    "    return np.where(uc > number_of_gestures)[0]\n",
    "\n",
    "frrs = []\n",
    "fars = []\n",
    "from scipy.stats import mode\n",
    "\n",
    "ws = 1\n",
    "def calculate_eer(res, y_test):\n",
    "    global frrs, fars\n",
    "    frrs = []\n",
    "    fars = []\n",
    "    end_index = max(np.where(y_test == 1)[0])\n",
    "    user_results = res[: end_index]\n",
    "    intruder_results = res[end_index:]\n",
    "\n",
    "    user_windows = user_results[(\n",
    "        np.expand_dims(np.arange(ws), 0) +\n",
    "        np.expand_dims(np.arange(len(user_results) - ws), 0).T\n",
    "    )]\n",
    "\n",
    "    intruder_windows = intruder_results[(\n",
    "            np.expand_dims(np.arange(ws), 0) +\n",
    "            np.expand_dims(np.arange(len(intruder_results) - ws), 0).T\n",
    "        )]\n",
    "\n",
    "    desc_scores = np.sort(res)[::-1]\n",
    "\n",
    "    for threshold in desc_scores[::10]:\n",
    "        FRR = 1 - np.mean(mode(user_windows >= threshold, axis=1)[0])\n",
    "        FAR = 1 - np.mean(mode(intruder_windows < threshold, axis=1)[0])\n",
    "\n",
    "        frrs.append(FRR)\n",
    "        fars.append(FAR)\n",
    "\n",
    "    fars = np.array(fars)\n",
    "    frrs = np.array(frrs)\n",
    "\n",
    "    eer = fars[np.argwhere(np.diff(np.sign(fars - frrs))).flatten()]\n",
    "    return eer[0] if eer.size > 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "DATASET = TOUCHALYTICS\n",
    "DATA_DIRECTORY = 'simple_cnn'\n",
    "\n",
    "resulting_data = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    with open(f'{DATA_DIRECTORY}/results/iteration_{iteration}.pkl', 'rb') as f:\n",
    "        u_training, u_testing, *_ = pickle.load(f)\n",
    "    USERS_USED_FOR_TRAINING_FEAT_EXTRACTOR = u_training\n",
    "    USERS_USED_FOR_TESTING_GENERALIZATION = u_testing\n",
    "\n",
    "    deep_model = models.load_model(f'{DATA_DIRECTORY}/models/simple_cnn_128_embedding_{iteration}.h5')\n",
    "    deep_model = models.Model(inputs = deep_model.input, outputs = deep_model.layers[-3].output)\n",
    "\n",
    "    WINDOW_SIZE = deep_model.input.shape[1]\n",
    "\n",
    "    if DATASET == BRAINRUN:\n",
    "        with open('brainrun_full_not_parsed.pkl', 'rb') as f:\n",
    "            users = pickle.load(f)\n",
    "            valid_users = []\n",
    "            for i, user in enumerate(users):\n",
    "                if i in USERS_USED_FOR_TESTING_GENERALIZATION:\n",
    "                    valid_users.append(user)\n",
    "\n",
    "            users = valid_users\n",
    "    if DATASET == TOUCHALYTICS:\n",
    "        with open('touchalytics_full_not_parsed.pkl', 'rb') as f:\n",
    "            users = pickle.load(f)\n",
    "        with open('touchalytics_full_not_parsed.pkl', 'rb') as f:\n",
    "            temp_users = pickle.load(f)\n",
    "\n",
    "    for user in tqdm(range(len(users))):\n",
    "        parse_user(user)\n",
    "        parse_user_dont_convert(user)\n",
    "\n",
    "    # if DATASET == TOUCHALYTICS:\n",
    "    valid_users = get_users_over_gestures(140)\n",
    "    break\n",
    "\n",
    "    # ceva('simple_cnn_results', iteration, valid_users)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "\"\"\"\n",
    "Python compute equal error rate (eer)\n",
    "ONLY tested on binary classification\n",
    "\n",
    ":param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    ":param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    ":param positive_label: the class that is viewed as positive class when computing EER\n",
    ":return: equal error rate (EER)\n",
    "\"\"\"\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    global fpr, fnr\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # the threshold of fnr == fpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    tresh = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    return (eer, tresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 10\n",
    "\n",
    "max_train_per_session = int(200000 / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "max_test_per_session = int(67777777 / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "X_test = np.concatenate([session[get_test_indices(session.shape[0], max_size=max_test_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "\n",
    "# Cross validation\n",
    "\n",
    "hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "\n",
    "best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "# results[user_id]['hyper'] = sorted(hyperparameters, key=lambda x: x[0])\n",
    "svm = OneClassSVM(\n",
    "            kernel = best_hyperparameters['kernel'], \n",
    "            nu=best_hyperparameters['nu'], \n",
    "            degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "            gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "res = np.concatenate([\n",
    "    svm.decision_function(scaler.transform(X_test)), *[\n",
    "        svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "])\n",
    "\n",
    "y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "intr_res = {}\n",
    "# Compute eer and treshold\n",
    "eer, threshold = compute_eer(y_test, res)\n",
    "print(eer)\n",
    "# Concatenate all other users data (from temp users) into one array\n",
    "possible_intruders = []\n",
    "for other_uid in valid_users[valid_users != user_id]:\n",
    "    for session in temp_users[other_uid]['devices'][0]['sessions']:\n",
    "        possible_intruders.extend(session['gestures'])\n",
    "\n",
    "# For each intruder 1-7 n_intruders\n",
    "for n_intruders in range(0, 11):\n",
    "#  intruder_list = Select X_test * intruder samples at random from the other users'data\n",
    "    intruders = np.random.choice(possible_intruders, size=n_intruders * X_test.shape[0], replace=False)\n",
    "    X_test_intruders = np.concatenate([session_to_dp_with_intruders(session, get_test_indices_custom(len(session['gestures']), max_size=max_test_per_session), n_intruders, intruders) for session in temp_users[int(user_id)]['devices'][0]['sessions']])\n",
    "    r_intr = svm.decision_function(scaler.transform(X_test_intruders))\n",
    "    print(np.mean(r_intr < threshold))\n",
    "    intr_res[n_intruders] = [r_intr, acc]\n",
    "# print(intr_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[0]['devices'][0]['sessions'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import OneClassSVM\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "MAX_TRAIN_SIZE = 40000\n",
    "MAX_TEST_SIZE = 10000\n",
    "\n",
    "def train_test_data(data, test_size = 0.2, gap = WINDOW_SIZE, max_size = 200):\n",
    "    # ERROR IS HERE\n",
    "    end = len(data)\n",
    "    middle = int(end * (1 - test_size))\n",
    "    return np.arange(middle - gap), np.arange(middle, end)\n",
    "\n",
    "def get_train_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    # ERROR IS HERE\n",
    "    size = size\n",
    "    middle = int(size * (1 - test_size))\n",
    "    middle = min(middle, max_size)\n",
    "    return np.arange(middle - gap)\n",
    "\n",
    "def get_test_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    return np.arange(middle, size)\n",
    "\n",
    "def get_test_indices_custom(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    return np.arange(middle - gap, size)\n",
    "    # return np.arange(0, size)\n",
    "\n",
    "def get_intruder_size(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    validation_middle = int(size * (1 - test_size / 2))\n",
    "    return np.arange(middle, validation_middle - gap), np.arange(validation_middle, size)  \n",
    "\n",
    "# Parameter space\n",
    "parameters = [{\n",
    "    'kernel': ['rbf'], 'gamma': [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001], 'nu': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],},\n",
    "    ]\n",
    "\n",
    "NUMBER_OF_G = 1000000\n",
    "\n",
    "def ceva(fp, iteration, valid_users):\n",
    "    FILE_PATH = f'{fp}/iteration_{iteration}'\n",
    "\n",
    "    def run_experiment_for_user(user_id, users, valid_users, parameters, temp_users):\n",
    "        # CHANGE HERE IF ERROR\n",
    "        max_train_per_session = int(NUMBER_OF_G / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        max_test_per_session = int(MAX_TRAIN_SIZE / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0], max_size=max_test_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        \n",
    "        # Cross validation\n",
    "\n",
    "        hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "       \n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        # results[user_id]['hyper'] = sorted(hyperparameters, key=lambda x: x[0])\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "        intr_res = {}\n",
    "        # Compute eer and treshold\n",
    "        eer, threshold = compute_eer(y_test, res)\n",
    "        print(eer)\n",
    "        # Concatenate all other users data (from temp users) into one array\n",
    "        possible_intruders = []\n",
    "        for other_uid in valid_users[valid_users != user_id]:\n",
    "            for session in temp_users[other_uid]['devices'][0]['sessions']:\n",
    "                possible_intruders.extend(session['gestures'])\n",
    "\n",
    "        # For each intruder 1-7 n_intruders\n",
    "        for n_intruders in range(1, 8):\n",
    "        #  intruder_list = Select X_test * intruder samples at random from the other users'data\n",
    "            intruders = np.random.choice(possible_intruders, size=n_intruders * X_test.shape[0], replace=False)\n",
    "            X_test = np.concatenate([session_to_dp_with_intruders(session, get_test_indices_custom(session.shape[0], max_size=max_test_per_session), n_intruders, intruders) for session in temp_users[int(user_id)]['devices'][0]['sessions']])\n",
    "            r_intr = svm.decision_function(scaler.transform(X_test))\n",
    "            acc = np.mean(r_intr > threshold)\n",
    "            intr_res[n_intruders] = [r_intr, acc]\n",
    "\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters, intr_res, USERS_USED_FOR_TESTING_GENERALIZATION), f)\n",
    "\n",
    "\n",
    "    def find_hyper(user_id, hyper_pair, users, valid_users, X_train, hyperparameters):\n",
    "        cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "        avg_eer = []\n",
    "        for train, test in cv.split(X_train):\n",
    "            clf = OneClassSVM(\n",
    "                kernel = hyper_pair['kernel'], \n",
    "                nu=hyper_pair['nu'], \n",
    "                degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "                gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "            scaler = StandardScaler()\n",
    "            clf.fit(scaler.fit_transform(X_train[train]))\n",
    "            res = np.concatenate([\n",
    "                clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "                    clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "            ])\n",
    "            y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "            \n",
    "            res[np.isneginf(res)] = -1000\n",
    "            res[np.isposinf(res)] = 1000\n",
    "            avg_eer.append(compute_eer(y_test, res))\n",
    "        hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "\n",
    "    # results = [{i: {}} for i in range(300)]\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # ERROR HERE\n",
    "    for user_id in list(set(valid_users).intersection([0,1])):\n",
    "        max_train_per_session = int(NUMBER_OF_G / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        max_test_per_session = int(MAX_TRAIN_SIZE / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0], max_size=max_test_per_session),] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        \n",
    "        # Cross validation\n",
    "\n",
    "        # hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "            \n",
    "        threads = []\n",
    "        cid = 0\n",
    "        N_THREADS = 32\n",
    "        can_exit = False\n",
    "\n",
    "        hyper_grid = list(ParameterGrid(parameters))\n",
    "        manager = mp.Manager()\n",
    "        hyperparameters = manager.list()\n",
    "\n",
    "        pbar = tqdm(total=len(hyper_grid))\n",
    "        while not can_exit:\n",
    "            while len(threads) < N_THREADS and cid < len(hyper_grid):\n",
    "                hyper_pair = hyper_grid[cid]\n",
    "                thread = mp.Process(target=find_hyper, args=(user_id, hyper_grid[cid], users, valid_users, X_train, hyperparameters))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "                pbar.update(1)\n",
    "                cid += 1\n",
    "\n",
    "            for thread in threads:\n",
    "                if not thread.is_alive():\n",
    "                    thread.join()\n",
    "                    threads.remove(thread)\n",
    "\n",
    "            if(len(threads) == 0):\n",
    "                can_exit = True\n",
    "            time.sleep(1)\n",
    "        pbar.close()\n",
    "        hyperparameters = list(hyperparameters)\n",
    "\n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        # results[user_id]['hyper'] = sorted(hyperparameters, key=lambda x: x[0])\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters, USERS_USED_FOR_TESTING_GENERALIZATION), f)\n",
    "\n",
    "        print(compute_eer(y_test, res))\n",
    "        # results[user_id]['eer'] = compute_eer(y_test, res)\n",
    "\n",
    "    # with open('1_class_svm_results_brainrun_full.pkl', 'wb') as f:\n",
    "    #     pickle.dump(results, f)\n",
    "        \n",
    "    # print('---------- RESULTS ----------')\n",
    "    # print(np.mean([x['eer'] for x in results]))\n",
    "    # print(np.mean([x['hyper'][0][0] for x in results]))\n",
    "\n",
    "\n",
    "    threads = []\n",
    "    cid = 0\n",
    "    N_THREADS = 32\n",
    "    can_exit = False\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # ERROR HERE\n",
    "    vu = list(set(valid_users).difference([0,1]))\n",
    "    pbar = tqdm(total=len(vu))\n",
    "    while not can_exit:\n",
    "        while len(threads) < N_THREADS and cid < len(vu):\n",
    "            user_id = vu[cid]\n",
    "            thread = mp.Process(target=run_experiment_for_user, args=(vu[cid], users, valid_users, parameters,))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "            cid += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                thread.join()\n",
    "                threads.remove(thread)\n",
    "\n",
    "        if(len(threads) == 0):\n",
    "            can_exit = True\n",
    "\n",
    "        time.sleep(1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecc757a38b814f9d9a78e27a3f0556e4d5be26951c473c897a83b93183395c00"
  },
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
