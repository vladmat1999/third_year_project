{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.display import JSON\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow import keras\n",
    "import pydot\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.draw import line_aa\n",
    "\n",
    "from IPython.display import Image \n",
    "\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_USERS = 10000\n",
    "MAX_SESSIONS_PER_USER = 10\n",
    "MIN_GESTURES_PER_SESSION = 100\n",
    "SOME_MAX_USERS = 2000\n",
    "MIN_GESTURES = 400\n",
    "NUM_PATHS = 10\n",
    "GESTURES = ['swipe']\n",
    "WINDOW_SIZE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current cases:\n",
    "# Use derived features (1)\n",
    "#     Change input shape to 91\n",
    "# Don't use derived features (2)\n",
    "#     Change input shape to 81\n",
    "INPUT_SHAPE = (20, 92, 1)\n",
    "USER_DERIVED_FEATURES = True\n",
    "TOUCHALYTICS = 'touchalytics'\n",
    "BRAINRUN = 'brainrun'\n",
    "DATASET = BRAINRUN\n",
    "USE_PRESSURE_AREA = False\n",
    "\n",
    "def configure_cases(case_number): \n",
    "    global INPUT_SHAPE, USER_DERIVED_FEATURES\n",
    "\n",
    "    if case_number == 1:\n",
    "        INPUT_SHAPE = (20, 92, 1)\n",
    "        USER_DERIVED_FEATURES = True\n",
    "    elif case_number == 2:\n",
    "        INPUT_SHAPE = (20, 81, 1)\n",
    "        USER_DERIVED_FEATURES = False\n",
    "\n",
    "configure_cases(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def pj(json):\n",
    "    print(dumps(json, indent = 4))\n",
    "\n",
    "def data_to_array(data):\n",
    "    return np.array([\n",
    "        data[k] for k in sorted(data.keys())\n",
    "    ])\n",
    "\n",
    "has_input = False\n",
    "\n",
    "def extract_features(data, delta_time, use_area_pressure = USE_PRESSURE_AREA):\n",
    "    result = []\n",
    "    points = [(data[0]['x0'], data[0]['y0'])] + [(x['moveX'], x['moveY']) for x in data]\n",
    "    points = np.array(points)\n",
    "\n",
    "    # # End to end distance - from paper (Touchalytics) and https://par.nsf.gov/servlets/purl/10167262\n",
    "    # result.append(np.linalg.norm(points[-1] - points[0]))\n",
    "    # # Path length - from Touchalytics adn https://par.nsf.gov/servlets/purl/10167262\n",
    "    # result.append(sum(np.linalg.norm(points[i] - points[i - 1]) for i in range(1, len(points))))\n",
    "    # # Path length / delta time - from https://par.nsf.gov/servlets/purl/10167262\n",
    "    # result.append(result[-1] / delta_time)\n",
    "    # # Average Acceleration\n",
    "    # # result.append(result[-1] / delta_time)\n",
    "\n",
    "    # # 20th percentile velocity - Touchalytics\n",
    "    # result.append(\n",
    "    #     np.percentile(\n",
    "    #         [np.linalg.norm(np.array([current_point['vx'], current_point['vy']])) for current_point in data],\n",
    "    #         20\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # # 80th percentile velocity - Touchalytics\n",
    "    # result.append(\n",
    "    #     np.percentile(\n",
    "    #         [np.linalg.norm(np.array([current_point['vx'], current_point['vy']])) for current_point in data],\n",
    "    #         80\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # # Angle (direction of end-to-end line) - Touchalytics\n",
    "    # result.append(np.angle(\n",
    "    #     complex(points[-1][0] - points[0][0], points[-1][1] - points[0][1])\n",
    "    # ))\n",
    "\n",
    "    # # Largest distance from end-to-end line (signed) - Touchalytics\n",
    "    # result.append(\n",
    "    #     np.max(\n",
    "    #         [np.cross(points[-1] - points[0], points[0] - p) / (np.linalg.norm(points[-1] - points[0]) or 1) for p in points]\n",
    "    #     ))\n",
    "\n",
    "    # # Horizontal mean position\n",
    "\n",
    "    # # Deviation from straight diagonal line\n",
    "    # # result.append(np.abs(data[-1]['moveX'] - data[0]['x0']) / np.abs(data[-1]['moveY'] - data[0]['y0']) if np.abs(data[-1]['moveY'] - data[0]['y0'] != 0 else 0)\n",
    "\n",
    "\n",
    "    # # Largest deviation from end to end line\n",
    "    # result.append(max(abs(np.arctan2(*(points[i] - points[i - 1]))) for i in range(1, len(points))))\n",
    "\n",
    "    # # First 3 point acceleration\n",
    "    # result.append(np.linalg.norm(2 * points[1] - points[0] - points[2]) / 2 if len(points) > 3 else 0)\n",
    "    # # Last 3 point acceleration\n",
    "    # result.append(np.linalg.norm(2 * points[-2] - points[-3] - points[-1]) / 2 if len(points) > 3 else 0)\n",
    "\n",
    "    # # Median velocity at last three points - Touchalytics (11 features + duration = 12 - without the first and last 3 points)\n",
    "    # result.append(\n",
    "    #     np.median(\n",
    "    #         np.array(\n",
    "    #             [np.linalg.norm(np.array([data[i]['vx'], data[i]['vy']])\n",
    "    #             ) for i in range(len(data) - 1, max(-1, len(data) - 4), -1)])))\n",
    "\n",
    "    # # Only for touchalytics\n",
    "    # if use_area_pressure:\n",
    "    #     # Midstroke area covered\n",
    "    #     result.append(data[int(len(data) // 2)]['area'])\n",
    "    #     # Midstroke pressure\n",
    "    #     result.append(data[int(len(data) // 2)]['pressure'])\n",
    "\n",
    "    res = np.nan_to_num(result)\n",
    "    first_three_points = points[:5].flatten() \n",
    "    last_three_points = points[-5:].flatten() \n",
    "    first_three_points.resize((10,))\n",
    "    last_three_points.resize((10,))\n",
    "\n",
    "    return np.concatenate([res, first_three_points, last_three_points])\n",
    "\n",
    "def draw_line(x0, y0, x1, y1, img_size, img, vel = 1):\n",
    "    rr,cc,val = line_aa(int(x0 * img_size), int(y0 * img_size), int(x1 * img_size), int(y1 * img_size))\n",
    "    if x0 > 1 or y0 > 1 or x1 > 1 or y1 > 1:\n",
    "        print(x0, y0, x1, y1)\n",
    "    img[rr,cc] = val * vel\n",
    "\n",
    "def points_to_image(points):\n",
    "    init = np.zeros((128, 128))\n",
    "\n",
    "    velocities = [np.linalg.norm(p1 - p2) for p1, p2 in zip(points[:-1], points[1:])]\n",
    "    velocities = (velocities - np.min(velocities)) / (np.ptp(velocities) or 1)\n",
    "    # print(velocities)\n",
    "    # draw_line(points[0][1], points[0][0], points[1][1], points[1][0], IMAGE_RESOLUTION[0] - 1, init, vel = velocities[0])\n",
    "\n",
    "    x0, y0 = points[0][1], points[0][0]\n",
    "    for datapoint, velocity in zip(points[1:], velocities):\n",
    "        draw_line(x0, y0, datapoint[1], datapoint[0], 128 - 1, init, vel = velocity)\n",
    "        x0, y0 = datapoint[1], datapoint[0]\n",
    "\n",
    "    res = init\n",
    "    res = cv2.resize(init, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "    mean, std = res.mean(), res.std()\n",
    "    res = (res - mean) / (std or 1)\n",
    "    res = res.reshape(32, 32, 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def gesture_to_image(c):\n",
    "    clip = lambda x: np.clip(x, 0, 1)\n",
    "    img_data = c['data']\n",
    "    \n",
    "    points = np.array([[clip(img_data[0]['x0']), clip(img_data[0]['y0'])]] + [[clip(pt['moveX']), clip(pt['moveY'])] for pt in img_data])\n",
    "\n",
    "    # draw_line(clip(img_data[0]['y0']), clip(img_data[0]['x0']), clip(img_data[0]['moveY']), clip(img_data[0]['moveX']) , IMAGE_RESOLUTION[0] - 1, init)\n",
    "    # for i in range(5):\n",
    "    #   noisy_points = noise_curve(points, std = 0.02)\n",
    "    #   noisy_points = [(clip(pt[0]), clip(pt[1])) for pt in noisy_points]\n",
    "    #   images.append(points_to_image(noisy_points))\n",
    "    # images = np.array(images)\n",
    "    # np.random.shuffle(images)\n",
    "    return points_to_image(points) \n",
    "    \n",
    "def gesture_to_data(c, use_extra_features = True):\n",
    "    # raw_data = [data_to_array(x) for x in c['data'][:NUM_PATHS]]\n",
    "    # Stroke duration\n",
    "    delta_time = (c['t_stop'] - c['t_start']) / 1000\n",
    "    extra_features = extract_features(c['data'], delta_time) if use_extra_features else []\n",
    "    # data_entry = np.pad(np.append([delta_time] + extra_features,\n",
    "            # np.array(raw_data),\n",
    "        # ), (0, max(0, (NUM_PATHS - len(c['data'])) * 8)), 'constant')\n",
    "    # data_entry = np.array([[x] for x in data_entry], dtype = np.float32)\n",
    "\n",
    "    # TODO REMEMBER TO REMOVE WHEN SLIDING WINDOW IS IMPLEMENTED \n",
    "    return np.concatenate([[c['t_start'], c['t_stop'], delta_time], extra_features])\n",
    "\n",
    "def window_to_datapoint(window):\n",
    "    # TODO CHECK AXIS\n",
    "    return np.sum(window, axis = 0) # Window start - previous window stop\n",
    "\n",
    "def session_to_datapoints(s):\n",
    "    featurized_session = np.array([gesture_to_image(x) for x in s['gestures']])\n",
    "    sliding_windows = (\n",
    "        np.expand_dims(np.arange(WINDOW_SIZE), 0) +\n",
    "        np.expand_dims(np.arange(len(featurized_session) - WINDOW_SIZE), 0).T\n",
    "    )\n",
    "\n",
    "    # return featurized_session[sliding_windows]\n",
    "\n",
    "    temp = np.array([window_to_datapoint(window) for window in featurized_session[sliding_windows]])\n",
    "    return deep_model.predict(temp)\n",
    "\n",
    "\n",
    "def session_to_dp_with_intruders(session, test_indices, n_intruders, intruders):\n",
    "    print(len(test_indices))\n",
    "    test_data = np.array([gesture_to_image(x) for x in session['gestures']])[test_indices]\n",
    "\n",
    "    # print(len(test_indices))\n",
    "    sliding_windows = (\n",
    "        np.expand_dims(np.arange(WINDOW_SIZE), 0) +\n",
    "        np.expand_dims(np.arange(len(test_data) - WINDOW_SIZE), 0).T\n",
    "    )\n",
    "\n",
    "    windows = test_data[sliding_windows]\n",
    "\n",
    "    intruders = np.array([gesture_to_image(x) for x in intruders])\n",
    "\n",
    "    # if n_intruders != 0:\n",
    "    #     windows[:, -n_intruders:, :] = intruders\n",
    "    temp = np.array([window_to_datapoint(np.concatenate([intruders[np.random.choice(intruders.shape[0], n_intruders)]])) for window in windows])\n",
    "    # print(temp[0])\n",
    "    # print('----------------------')\n",
    "    return deep_model.predict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "LIMIT_GESTURES_PER_USER = 4000\n",
    "SESSION_GESTURES_MORE_THAN = 140\n",
    "good_screens = ['MathisisGame', 'FocusGame']\n",
    "\n",
    "def prefilter_session(s):\n",
    "    s['gestures'].sort(key = lambda x: x['t_start'])\n",
    "    s['gestures'] = [x for x in s['gestures'] \n",
    "        if x['t_stop'] - x['t_start'] > 70 and x['t_stop'] - x['t_start'] < (1000 if DATASET == BRAINRUN else 2000) and \n",
    "        ((x['screen'].split(' ')[0] in good_screens and x['type'] == 'swipe') if DATASET == BRAINRUN else True)]\n",
    "\n",
    "def parse_user(user_id):\n",
    "    i = 0\n",
    "\n",
    "    total = 0\n",
    "    should_delete = False\n",
    "    while i < len(users[user_id]['devices'][0]['sessions']):\n",
    "        prefilter_session(users[user_id]['devices'][0]['sessions'][i])\n",
    "\n",
    "        if len(users[user_id]['devices'][0]['sessions'][i]['gestures']) < SESSION_GESTURES_MORE_THAN or should_delete:\n",
    "            del users[user_id]['devices'][0]['sessions'][i]\n",
    "        else:\n",
    "            users[user_id]['devices'][0]['sessions'][i] = session_to_datapoints(users[user_id]['devices'][0]['sessions'][i])\n",
    "            # Outlier detection\n",
    "            # clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "            # users[user_id]['devices'][0]['sessions'][i] = \\\n",
    "            #     users[user_id]['devices'][0]['sessions'][i][np.where(clf.fit_predict(users[user_id]['devices'][0]['sessions'][i]) == 1)]\n",
    "            total += len(users[user_id]['devices'][0]['sessions'][i])\n",
    "            i += 1\n",
    "    # # Order sessions by length ascending, trim sessions to specific length based on gestures allowed\n",
    "    # inx_len = np.argsort([len(x) for x in users[user_id]['devices'][0]['sessions']])\n",
    "    # sessions_remaining = len(users[user_id]['devices'][0]['sessions'])\n",
    "    # gestures_remaining = LIMIT_GESTURES_PER_USER\n",
    "    # for i in inx_len:\n",
    "    #     gestures_this_session = int(gestures_remaining / sessions_remaining)\n",
    "    #     gestures_remaining -= gestures_this_session\n",
    "    #     users[user_id]['devices'][0]['sessions'][i] = users[user_id]['devices'][0]['sessions'][i][:gestures_this_session]\n",
    "    #     sessions_remaining -= 1\n",
    "\n",
    "def parse_user_dont_convert(user_id):\n",
    "    i = 0\n",
    "    total = 0\n",
    "    should_delete = False\n",
    "    while i < len(temp_users[user_id]['devices'][0]['sessions']):\n",
    "        prefilter_session(temp_users[user_id]['devices'][0]['sessions'][i])\n",
    "\n",
    "        if len(temp_users[user_id]['devices'][0]['sessions'][i]['gestures']) < SESSION_GESTURES_MORE_THAN or should_delete:\n",
    "            del temp_users[user_id]['devices'][0]['sessions'][i]\n",
    "        else:\n",
    "            total += len(temp_users[user_id]['devices'][0]['sessions'][i])\n",
    "            i += 1\n",
    "\n",
    "def get_users_over_gestures(number_of_gestures = 600):\n",
    "    uc = np.zeros((len(users), ))\n",
    "    for i in range(len(users)):\n",
    "        uc[i] = 0\n",
    "        for session in users[i]['devices'][0]['sessions']:\n",
    "            uc[i] += session.shape[0]\n",
    "\n",
    "    return np.where(uc > number_of_gestures)[0]\n",
    "\n",
    "frrs = []\n",
    "fars = []\n",
    "from scipy.stats import mode\n",
    "\n",
    "ws = 1\n",
    "def calculate_eer(res, y_test):\n",
    "    global frrs, fars\n",
    "    frrs = []\n",
    "    fars = []\n",
    "    end_index = max(np.where(y_test == 1)[0])\n",
    "    user_results = res[: end_index]\n",
    "    intruder_results = res[end_index:]\n",
    "\n",
    "    user_windows = user_results[(\n",
    "        np.expand_dims(np.arange(ws), 0) +\n",
    "        np.expand_dims(np.arange(len(user_results) - ws), 0).T\n",
    "    )]\n",
    "\n",
    "    intruder_windows = intruder_results[(\n",
    "            np.expand_dims(np.arange(ws), 0) +\n",
    "            np.expand_dims(np.arange(len(intruder_results) - ws), 0).T\n",
    "        )]\n",
    "\n",
    "    desc_scores = np.sort(res)[::-1]\n",
    "\n",
    "    for threshold in desc_scores[::10]:\n",
    "        FRR = 1 - np.mean(mode(user_windows >= threshold, axis=1)[0])\n",
    "        FAR = 1 - np.mean(mode(intruder_windows < threshold, axis=1)[0])\n",
    "\n",
    "        frrs.append(FRR)\n",
    "        fars.append(FAR)\n",
    "\n",
    "    fars = np.array(fars)\n",
    "    frrs = np.array(frrs)\n",
    "\n",
    "    eer = fars[np.argwhere(np.diff(np.sign(fars - frrs))).flatten()]\n",
    "    return eer[0] if eer.size > 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "DATASET = TOUCHALYTICS\n",
    "DATA_DIRECTORY = 'image_cnn_ws_11'\n",
    "\n",
    "resulting_data = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    with open(f'{DATA_DIRECTORY}/results/iteration_{iteration}.pkl', 'rb') as f:\n",
    "        u_training, u_testing, *_ = pickle.load(f)\n",
    "    USERS_USED_FOR_TRAINING_FEAT_EXTRACTOR = u_training\n",
    "    USERS_USED_FOR_TESTING_GENERALIZATION = u_testing\n",
    "\n",
    "    deep_model = models.load_model(f'{DATA_DIRECTORY}/models/simple_cnn_128_embedding_{iteration}.h5')\n",
    "    deep_model = models.Model(inputs = deep_model.input, outputs = deep_model.layers[-3].output)\n",
    "\n",
    "    WINDOW_SIZE = 11\n",
    "\n",
    "    if DATASET == BRAINRUN:\n",
    "        with open('brainrun_full_not_parsed.pkl', 'rb') as f:\n",
    "            users = pickle.load(f)\n",
    "            valid_users = []\n",
    "            for i, user in enumerate(users):\n",
    "                if i in USERS_USED_FOR_TESTING_GENERALIZATION:\n",
    "                    valid_users.append(user)\n",
    "\n",
    "            users = valid_users\n",
    "    if DATASET == TOUCHALYTICS:\n",
    "        with open('touchalytics_full_not_parsed.pkl', 'rb') as f:\n",
    "            users = pickle.load(f)\n",
    "        with open('touchalytics_full_not_parsed.pkl', 'rb') as f:\n",
    "            temp_users = pickle.load(f)\n",
    "\n",
    "    for user in tqdm(range(len(users))):\n",
    "        parse_user(user)\n",
    "        parse_user_dont_convert(user)\n",
    "\n",
    "    # if DATASET == TOUCHALYTICS:\n",
    "    valid_users = get_users_over_gestures(140)\n",
    "    break\n",
    "\n",
    "    # ceva('simple_cnn_results', iteration, valid_users)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "\"\"\"\n",
    "Python compute equal error rate (eer)\n",
    "ONLY tested on binary classification\n",
    "\n",
    ":param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    ":param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    ":param positive_label: the class that is viewed as positive class when computing EER\n",
    ":return: equal error rate (EER)\n",
    "\"\"\"\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    global fpr, fnr\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # the threshold of fnr == fpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    tresh = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    return (eer, tresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "\n",
    "max_train_per_session = int(200000 / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "max_test_per_session = int(67777777 / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "X_test = np.concatenate([session[get_test_indices(session.shape[0], max_size=max_test_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "\n",
    "# Cross validation\n",
    "\n",
    "hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "# ERROR UNCOMMENT THIS\n",
    "# for hyper_pair in ParameterGrid(parameters):\n",
    "#     cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "#     avg_eer = []\n",
    "#     for train, test in cv.split(X_train):\n",
    "#         clf = OneClassSVM(\n",
    "#             kernel = hyper_pair['kernel'], \n",
    "#             nu=hyper_pair['nu'], \n",
    "#             degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "#             gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "#         scaler = StandardScaler()\n",
    "#         clf.fit(scaler.fit_transform(X_train[train]))\n",
    "#         res = np.concatenate([\n",
    "#             clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "#                 clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "#         ])\n",
    "#         y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "        \n",
    "#         res[np.isneginf(res)] = -1000\n",
    "#         res[np.isposinf(res)] = 1000\n",
    "#         eer, thresh = compute_eer(y_test, res)\n",
    "#         avg_eer.append(eer)\n",
    "#     hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "\n",
    "best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "# results[user_id]['hyper'] = sorted(hyperparameters, key=lambda x: x[0])\n",
    "svm = OneClassSVM(\n",
    "            kernel = best_hyperparameters['kernel'], \n",
    "            nu=best_hyperparameters['nu'], \n",
    "            degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "            gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "res = np.concatenate([\n",
    "    svm.decision_function(scaler.transform(X_test)), *[\n",
    "        svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "])\n",
    "\n",
    "y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "intr_res = {}\n",
    "# Compute eer and treshold\n",
    "eer, threshold = compute_eer(y_test, res)\n",
    "print(eer)\n",
    "# Concatenate all other users data (from temp users) into one array\n",
    "possible_intruders = []\n",
    "for other_uid in valid_users[valid_users != user_id]:\n",
    "    for session in temp_users[other_uid]['devices'][0]['sessions']:\n",
    "        possible_intruders.extend(session['gestures'])\n",
    "\n",
    "# For each intruder 1-7 n_intruders\n",
    "for n_intruders in range(1, 11):\n",
    "#  intruder_list = Select X_test * intruder samples at random from the other users'data\n",
    "    intruders = np.random.choice(possible_intruders, size=n_intruders * X_test.shape[0], replace=False)\n",
    "    X_test_intruders = np.concatenate([session_to_dp_with_intruders(session, get_test_indices_custom(len(session['gestures']), max_size=max_test_per_session), n_intruders, intruders) for session in temp_users[int(user_id)]['devices'][0]['sessions']])\n",
    "    print(X_test.shape)\n",
    "    r_intr = svm.decision_function(scaler.transform(X_test_intruders))\n",
    "    acc = np.mean(r_intr > threshold)\n",
    "    print(acc, np.mean(svm.decision_function(scaler.transform(X_test)) > threshold))\n",
    "    intr_res[n_intruders] = [r_intr, acc]\n",
    "# print(intr_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import OneClassSVM\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "MAX_TRAIN_SIZE = 40000\n",
    "MAX_TEST_SIZE = 10000\n",
    "\n",
    "def train_test_data(data, test_size = 0.2, gap = WINDOW_SIZE, max_size = 200):\n",
    "    # ERROR IS HERE\n",
    "    end = len(data)\n",
    "    middle = int(end * (1 - test_size))\n",
    "    return np.arange(middle - gap), np.arange(middle, end)\n",
    "\n",
    "def get_train_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    # ERROR IS HERE\n",
    "    size = size\n",
    "    middle = int(size * (1 - test_size))\n",
    "    middle = min(middle, max_size)\n",
    "    return np.arange(middle - gap)\n",
    "\n",
    "def get_test_indices(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    return np.arange(middle, size)\n",
    "\n",
    "def get_test_indices_custom(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    return np.arange(middle, size)\n",
    "    # return np.arange(0, size)\n",
    "\n",
    "def get_intruder_size(size, test_size = 0.2, gap = WINDOW_SIZE, max_size = 500):\n",
    "    size = min(size, max_size)\n",
    "    middle = int(size * (1 - test_size))\n",
    "    validation_middle = int(size * (1 - test_size / 2))\n",
    "    return np.arange(middle, validation_middle - gap), np.arange(validation_middle, size)  \n",
    "\n",
    "# X, y = [], []\n",
    "# valid_users_true = get_users_over_gestures(140)\n",
    "# valid_users = get_users_over_gestures(140)\n",
    "\n",
    "# for u in valid_users:\n",
    "#     for session in users[int(u)]['devices'][0]['sessions']:\n",
    "#         X.append(session)\n",
    "#         y.append(np.zeros((session.shape[0], )) + int(u))\n",
    "\n",
    "# Parameter space\n",
    "parameters = [{\n",
    "    'kernel': ['rbf'], 'gamma': [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001], 'nu': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],},\n",
    "    # 'kernel': ['rbf'], 'gamma': [0.00001], 'nu': [1.0],},\n",
    "    # {'kernel': ['poly'], 'degree': [1, 3, 9], 'nu': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],'gamma': [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]},\n",
    "    ]\n",
    "\n",
    "NUMBER_OF_G = 1000000\n",
    "\n",
    "def ceva(fp, iteration, valid_users):\n",
    "    FILE_PATH = f'{fp}/iteration_{iteration}'\n",
    "\n",
    "    def run_experiment_for_user(user_id, users, valid_users, parameters, temp_users):\n",
    "        # CHANGE HERE IF ERROR\n",
    "        max_train_per_session = int(NUMBER_OF_G / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        max_test_per_session = int(MAX_TRAIN_SIZE / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0], max_size=max_test_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        \n",
    "        # Cross validation\n",
    "\n",
    "        hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "        # ERROR UNCOMMENT THIS\n",
    "        # for hyper_pair in ParameterGrid(parameters):\n",
    "        #     cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "        #     avg_eer = []\n",
    "        #     for train, test in cv.split(X_train):\n",
    "        #         clf = OneClassSVM(\n",
    "        #             kernel = hyper_pair['kernel'], \n",
    "        #             nu=hyper_pair['nu'], \n",
    "        #             degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "        #             gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "        #         scaler = StandardScaler()\n",
    "        #         clf.fit(scaler.fit_transform(X_train[train]))\n",
    "        #         res = np.concatenate([\n",
    "        #             clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "        #                 clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        #         ])\n",
    "        #         y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "                \n",
    "        #         res[np.isneginf(res)] = -1000\n",
    "        #         res[np.isposinf(res)] = 1000\n",
    "        #         eer, thresh = compute_eer(y_test, res)\n",
    "        #         avg_eer.append(eer)\n",
    "        #     hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "\n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        # results[user_id]['hyper'] = sorted(hyperparameters, key=lambda x: x[0])\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "\n",
    "        intr_res = {}\n",
    "        # Compute eer and treshold\n",
    "        eer, threshold = compute_eer(y_test, res)\n",
    "        print(eer)\n",
    "        # Concatenate all other users data (from temp users) into one array\n",
    "        possible_intruders = []\n",
    "        for other_uid in valid_users[valid_users != user_id]:\n",
    "            for session in temp_users[other_uid]['devices'][0]['sessions']:\n",
    "                possible_intruders.extend(session['gestures'])\n",
    "\n",
    "        # For each intruder 1-7 n_intruders\n",
    "        for n_intruders in range(1, 8):\n",
    "        #  intruder_list = Select X_test * intruder samples at random from the other users'data\n",
    "            intruders = np.random.choice(possible_intruders, size=n_intruders * X_test.shape[0], replace=False)\n",
    "            X_test = np.concatenate([session_to_dp_with_intruders(session, get_test_indices_custom(session.shape[0], max_size=max_test_per_session), n_intruders, intruders) for session in temp_users[int(user_id)]['devices'][0]['sessions']])\n",
    "            r_intr = svm.decision_function(scaler.transform(X_test))\n",
    "            acc = np.mean(r_intr > threshold)\n",
    "            intr_res[n_intruders] = [r_intr, acc]\n",
    "\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters, intr_res, USERS_USED_FOR_TESTING_GENERALIZATION), f)\n",
    "\n",
    "        # print(compute_eer(y_test, res))\n",
    "\n",
    "\n",
    "        # results[user_id]['eer'] = compute_eer(y_test, res)\n",
    "\n",
    "    def find_hyper(user_id, hyper_pair, users, valid_users, X_train, hyperparameters):\n",
    "        cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "        avg_eer = []\n",
    "        for train, test in cv.split(X_train):\n",
    "            clf = OneClassSVM(\n",
    "                kernel = hyper_pair['kernel'], \n",
    "                nu=hyper_pair['nu'], \n",
    "                degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "                gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "            scaler = StandardScaler()\n",
    "            clf.fit(scaler.fit_transform(X_train[train]))\n",
    "            res = np.concatenate([\n",
    "                clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "                    clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "            ])\n",
    "            y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "            \n",
    "            res[np.isneginf(res)] = -1000\n",
    "            res[np.isposinf(res)] = 1000\n",
    "            avg_eer.append(compute_eer(y_test, res))\n",
    "        hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "\n",
    "    # results = [{i: {}} for i in range(300)]\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # ERROR HERE\n",
    "    for user_id in list(set(valid_users).intersection([0,1])):\n",
    "        max_train_per_session = int(NUMBER_OF_G / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        max_test_per_session = int(MAX_TRAIN_SIZE / len(users[int(user_id)]['devices'][0]['sessions']))\n",
    "        X_train = np.concatenate([session[get_train_indices(session.shape[0], max_size=max_train_per_session)] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        X_test = np.concatenate([session[get_test_indices(session.shape[0], max_size=max_test_per_session),] for session in users[int(user_id)]['devices'][0]['sessions']])\n",
    "        \n",
    "        # Cross validation\n",
    "\n",
    "        # hyperparameters = [(100, {'kernel':'rbf', 'gamma':0.8, 'nu': 0.3})]\n",
    "            \n",
    "        threads = []\n",
    "        cid = 0\n",
    "        N_THREADS = 32\n",
    "        can_exit = False\n",
    "\n",
    "        hyper_grid = list(ParameterGrid(parameters))\n",
    "        manager = mp.Manager()\n",
    "        hyperparameters = manager.list()\n",
    "\n",
    "        pbar = tqdm(total=len(hyper_grid))\n",
    "        while not can_exit:\n",
    "            while len(threads) < N_THREADS and cid < len(hyper_grid):\n",
    "                hyper_pair = hyper_grid[cid]\n",
    "                thread = mp.Process(target=find_hyper, args=(user_id, hyper_grid[cid], users, valid_users, X_train, hyperparameters))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "                pbar.update(1)\n",
    "                cid += 1\n",
    "\n",
    "            for thread in threads:\n",
    "                if not thread.is_alive():\n",
    "                    thread.join()\n",
    "                    threads.remove(thread)\n",
    "\n",
    "            if(len(threads) == 0):\n",
    "                can_exit = True\n",
    "            time.sleep(1)\n",
    "        pbar.close()\n",
    "        hyperparameters = list(hyperparameters)\n",
    "        # for hyper_pair in ParameterGrid(parameters):\n",
    "        #     cv = TimeSeriesSplit(n_splits=4, gap = WINDOW_SIZE)\n",
    "        #     avg_eer = []\n",
    "        #     for train, test in tqdm(cv.split(X_train)):\n",
    "        #         clf = OneClassSVM(\n",
    "        #             kernel = hyper_pair['kernel'], \n",
    "        #             nu=hyper_pair['nu'], \n",
    "        #             degree = hyper_pair['degree'] if 'degree' in hyper_pair else 0, \n",
    "        #             gamma = hyper_pair['gamma'] if 'gamma' in hyper_pair else 0,)\n",
    "        #         scaler = StandardScaler()\n",
    "        #         clf.fit(scaler.fit_transform(X_train[train]))\n",
    "        #         res = np.concatenate([\n",
    "        #             clf.decision_function(scaler.transform(X_train[test])), *[\n",
    "        #                 clf.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[0]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        #         ])\n",
    "        #         y_test = np.concatenate([np.zeros((test.shape[0],)) + 1, np.zeros((res.shape[0] - test.shape[0],)) - 1])\n",
    "                \n",
    "        #         res[np.isneginf(res)] = -1000\n",
    "        #         res[np.isposinf(res)] = 1000\n",
    "        #         avg_eer.append(compute_eer(y_test, res))\n",
    "        #     hyperparameters.append((np.mean(avg_eer), hyper_pair))\n",
    "\n",
    "        best_hyperparameters = sorted(hyperparameters, key=lambda x: x[0])[0][1]\n",
    "        # results[user_id]['hyper'] = sorted(hyperparameters, key=lambda x: x[0])\n",
    "        svm = OneClassSVM(\n",
    "                    kernel = best_hyperparameters['kernel'], \n",
    "                    nu=best_hyperparameters['nu'], \n",
    "                    degree = best_hyperparameters['degree'] if 'degree' in best_hyperparameters else 0, \n",
    "                    gamma = best_hyperparameters['gamma'] if 'gamma' in best_hyperparameters else 0,)\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        svm.fit(scaler.fit_transform(X_train))\n",
    "\n",
    "        res = np.concatenate([\n",
    "            svm.decision_function(scaler.transform(X_test)), *[\n",
    "                svm.decision_function(scaler.transform(session[get_intruder_size(session.shape[0])[1]])) for other_uid in valid_users[valid_users != user_id] for session in users[other_uid]['devices'][0]['sessions']]\n",
    "        ])\n",
    "\n",
    "        y_test = np.concatenate([np.zeros((X_test.shape[0],)) + 1, np.zeros((res.shape[0] - X_test.shape[0],)) - 1])\n",
    "        # Save results with pickle to a file\n",
    "        with open(f'{FILE_PATH}/user_{user_id}.pkl', 'wb') as f:\n",
    "            pickle.dump((y_test, res, hyperparameters, USERS_USED_FOR_TESTING_GENERALIZATION), f)\n",
    "\n",
    "        print(compute_eer(y_test, res))\n",
    "        # results[user_id]['eer'] = compute_eer(y_test, res)\n",
    "\n",
    "    # with open('1_class_svm_results_brainrun_full.pkl', 'wb') as f:\n",
    "    #     pickle.dump(results, f)\n",
    "        \n",
    "    # print('---------- RESULTS ----------')\n",
    "    # print(np.mean([x['eer'] for x in results]))\n",
    "    # print(np.mean([x['hyper'][0][0] for x in results]))\n",
    "\n",
    "\n",
    "    threads = []\n",
    "    cid = 0\n",
    "    N_THREADS = 32\n",
    "    can_exit = False\n",
    "\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        os.makedirs(FILE_PATH)\n",
    "\n",
    "    # ERROR HERE\n",
    "    vu = list(set(valid_users).difference([0,1]))\n",
    "    pbar = tqdm(total=len(vu))\n",
    "    while not can_exit:\n",
    "        while len(threads) < N_THREADS and cid < len(vu):\n",
    "            user_id = vu[cid]\n",
    "            thread = mp.Process(target=run_experiment_for_user, args=(vu[cid], users, valid_users, parameters,))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "            cid += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                thread.join()\n",
    "                threads.remove(thread)\n",
    "\n",
    "        if(len(threads) == 0):\n",
    "            can_exit = True\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# valid_users = valid_users_true\n",
    "# ceva()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calculate_eer(res, y_test)\n",
    "\n",
    "# end_index = max(np.where(y_test == 1)[0])\n",
    "\n",
    "# user_results = res[: end_index]\n",
    "# intruder_results = res[end_index:]\n",
    "\n",
    "# user_windows = user_results[(\n",
    "#         np.expand_dims(np.arange(10), 0) +\n",
    "#         np.expand_dims(np.arange(len(user_results) - 10), 0).T\n",
    "#     )]\n",
    "\n",
    "# intruder_windows = intruder_results[(\n",
    "#         np.expand_dims(np.arange(10), 0) +\n",
    "#         np.expand_dims(np.arange(len(intruder_results) - 10), 0).T\n",
    "#     )]\n",
    "\n",
    "# test = user_windows[:2, :]\n",
    "\n",
    "# np.mean((res[800:]) < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/maj_vote/user_0.pkl', 'rb') as f:\n",
    "    y_test, res = pickle.load(f)\n",
    "\n",
    "compute_eer(y_test, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, frrs.__len__(), 1), frrs, label='FRR')\n",
    "plt.plot(np.arange(0, fars.__len__(), 1), fars, label='FAR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, fpr.__len__(), 1), fnr, label='FRR')\n",
    "plt.plot(np.arange(0, fnr.__len__(), 1), fpr, label='FAR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "to_plot = []\n",
    "for i in [1,2,3,5,7,11]:\n",
    "    with open(f'results/1_classsvm_touchalytics_test_split_last_window_size_{i}.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    plt.plot([x['eer'] *100 for x in results], color = list(mcolors.TABLEAU_COLORS.keys())[i // 2], label=f'window size {i}', zorder=100, alpha=0.5)\n",
    "    to_plot.append([x['eer'] for x in results[:40]])\n",
    "\n",
    "to_plot = np.array(to_plot)\n",
    "plt.plot(np.mean(to_plot, axis=0) * 100, color='black', label='mean', zorder=200, linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import math\n",
    "\n",
    "features_dict = {\n",
    "    'end_to_end_distance': 0,\n",
    "    'path_length': 1,\n",
    "    'path_length/time': 2,\n",
    "    'avg acceleration': 3,\n",
    "    'Deviation from straight horizontal line': 4,\n",
    "    'Deviation from straight vertical line': 5,\n",
    "    'Angle': 6,\n",
    "    'Largest deviation from end-to-end line': 7,\n",
    "    'First 3 point acc': 8,\n",
    "    'Last 3 point acc': 9,\n",
    "    'Median last 5 point acceleration': 10,\n",
    "    'x0': 11,\n",
    "    'y0': 12,\n",
    "    'x1': 13,\n",
    "    'y1': 14,\n",
    "    'x2': 15,\n",
    "    'y2': 16,\n",
    "    'x3': 17,\n",
    "    'y3': 18,\n",
    "    'x4': 19,\n",
    "    'y4': 20,\n",
    "    'delta_time': 21,\n",
    "    'time_between_gestures': 22,\n",
    "    'gesture_time': 23\n",
    "    }\n",
    "\n",
    "n_rows = int(len(features_dict) ** 0.5)\n",
    "n_columns = math.ceil((len(features_dict)) / n_rows)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_columns, figsize = (20, 20))\n",
    "\n",
    "for name, index in features_dict.items():\n",
    "    for i in range(0, 20, 2):\n",
    "        axes[index//n_columns][index%n_columns].hist((data[i][np.all([data[i][:, 22] < 5000, data[i][:,21] < 1000, data[i][:,3] < 0.0003], axis = 0)])[:,index], bins=40, color = list(mcolors.TABLEAU_COLORS.keys())[i//2])  # density=False would make counts\n",
    "        axes[index//n_columns][index%n_columns].set_title(name)\n",
    "# density=False would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "good_data = np.concatenate([data[i] for i in range(0,40,2)]).astype(np.float32)\n",
    "\n",
    "df = pd.DataFrame(good_data)\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    df.corr(), \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.color_palette(\"crest\", as_cmap=True),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred = lof.fit_predict(np.concatenate(data[::2]))\n",
    "data_cleaned = np.concatenate(np.array(data[::2]))\n",
    "data_cleaned = data_cleaned[y_pred == 1]\n",
    "data_cleaned = data_cleaned[data_cleaned[:, 3] < 0.0003]\n",
    "print(data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_columns, figsize = (20, 20))\n",
    "\n",
    "for name, index in features_dict.items():\n",
    "        axes[index//n_columns][index%n_columns].hist(data_cleaned[:,index], density=True, bins=100, color = list(mcolors.TABLEAU_COLORS.keys())[0])  # density=False would make counts\n",
    "        axes[index//n_columns][index%n_columns].set_title(name)\n",
    "# density=False would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "clf = OneClassSVM(kernel='poly', degree = 1)\n",
    "td = np.array([data[2][i:i+40].flatten() for i in range(0, len(data[2][:700]) - 40)])\n",
    "clf = clf.fit(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(clf.predict(np.array([data[2][i:i+40].flatten() for i in range(0, len(data[2][:700]) - 40)])) == 1) / len(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20)\n",
    "principalComponents = pca.fit_transform(X_std)\n",
    "# # Plot the explained variances\n",
    "# features = range(pca.n_components_)\n",
    "# plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "# plt.xlabel('PCA features')\n",
    "# plt.ylabel('variance %')\n",
    "# plt.xticks(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(principalComponents[:, 0], principalComponents[:, 1], alpha=0.01, color='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 20)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(principalComponents)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecc757a38b814f9d9a78e27a3f0556e4d5be26951c473c897a83b93183395c00"
  },
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
